{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Lexical(어휘) Resources\n",
    "\n",
    "lexicon, lexical resource 는 어떠한 단어, 문장에 대한 정보를 가진 것들의 집합이다. 예를 들어 vocab=sorted(set(my_text)) 와 word_freq=FreqDist(my_text) 모두 lexical resource 이다. \n",
    "\n",
    "<img src=\"picture/캡처3.JPG\" width=600 />\n",
    "\n",
    "A <strong>lexical entry</strong> consists of a <strong>headword</strong> (also known as a lemma) along with additional information, such as the part-of-speech(품사) and the sense definition. Two distinct words having the same spelling are called\n",
    "<strong>homonyms.</strong>\n",
    "\n",
    "lexical entry는 품사나 단어의 의미같은 headword로 구성되어 있다. 같은 스펠링을 가지는 두 다른 단어는 homonym 이라고 불린다.\n",
    "\n",
    "NLTK의 rexical resource를 살펴보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordlist Corpora\n",
    "\n",
    "NLTK 는 워드의 리스트만으로 구성된 코퍼스들을 포함하고 있따. Words 코퍼스는 유닉스의 /usr/dict/words 의 스펠링체크를 위한 파일이다. 우리는 이 코퍼스를 잘못된 스펠링을 찾기 위해 사용할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:64: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if 'order' in inspect.getargspec(np.copy)[0]:\n"
     ]
    }
   ],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en', 'en-basic']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.words.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'a',\n",
       " 'aa',\n",
       " 'aal',\n",
       " 'aalii',\n",
       " 'aam',\n",
       " 'Aani',\n",
       " 'aardvark',\n",
       " 'aardwolf',\n",
       " 'Aaron',\n",
       " 'Aaronic',\n",
       " 'Aaronical',\n",
       " 'Aaronite',\n",
       " 'Aaronitic',\n",
       " 'Aaru',\n",
       " 'Ab',\n",
       " 'aba',\n",
       " 'Ababdeh',\n",
       " 'Ababua',\n",
       " 'abac']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.words.words()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unusual_word 함수를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unusual_words(text):\n",
    "    text_vocab = set(w.lower() for w in text if w.isalpha())\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    unusual = text_vocab.difference(english_vocab)\n",
    "    return sorted(unusual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### isalpha() 는 공백이나 특수 문자가 있는지 체크\n",
    "- #### difference는 차집합 set 인스턴스의 메서드인듯 싶다\n",
    "- #### sorted는 정렬해주는 함수 list.sort() 를 많이 봤었는데.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "str = \"this\";  # No space & digit in this string\n",
    "print (str.isalpha())\n",
    "\n",
    "str = \"this is string example....wow!!!\";\n",
    "print (str.isalpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2', '3'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([\"1\",\"2\",\"3\"]).difference(set(['1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([1,3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abbeyland',\n",
       " 'abhorred',\n",
       " 'abilities',\n",
       " 'abounded',\n",
       " 'abridgement',\n",
       " 'abused',\n",
       " 'abuses',\n",
       " 'accents',\n",
       " 'accepting',\n",
       " 'accommodations',\n",
       " 'accompanied',\n",
       " 'accounted',\n",
       " 'accounts',\n",
       " 'accustomary',\n",
       " 'aches',\n",
       " 'acknowledging',\n",
       " 'acknowledgment',\n",
       " 'acknowledgments',\n",
       " 'acquaintances',\n",
       " 'acquiesced']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unusual_words(nltk.corpus.gutenberg.words('austen-sense.txt'))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaaaaaaaaaaaaaaaa',\n",
       " 'aaahhhh',\n",
       " 'abortions',\n",
       " 'abou',\n",
       " 'abourted',\n",
       " 'abs',\n",
       " 'ack',\n",
       " 'acros',\n",
       " 'actualy',\n",
       " 'adams',\n",
       " 'adds',\n",
       " 'adduser',\n",
       " 'adjusts',\n",
       " 'adoted',\n",
       " 'adreniline',\n",
       " 'ads',\n",
       " 'adults',\n",
       " 'afe',\n",
       " 'affairs',\n",
       " 'affari']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unusual_words(nltk.corpus.nps_chat.words())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>stopwords</strong> 이라는 코퍼스도 존재하는데 이 코퍼스는 the, to , also 같이 너무 자주 쓰여서 분석을 위해 제거해야 하는 단어들이다. stopwords 는 적은 lexical content 들을 가지고 있고 이들은 분석에 방해되므로 제거해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stopwords가 얼마나 포함되어 있는지 살펴보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_fraction(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return len(content) / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.735240435097661"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_fraction(nltk.corpus.reuters.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stopwords를 사용해서 텍스트의 30%정도를 필터링 할 수 있었다. 여기서 주목해야 할것은 lexical resource를 이용해서 content 코퍼스를 필터링 한 것이다.\n",
    "\n",
    "### word puzzle\n",
    "\n",
    "<img src=\"picture/캡처4.JPG\" width=600 />\n",
    "\n",
    "위의 단어들로 주어진 조건들을 만족하는 단어를 찾는 퍼즐\n",
    "\n",
    "1. R을 무조건 포함하는 단어\n",
    "2. 위의 단어는 하나씩만 포함해야한다.\n",
    "3. 글자수가 6보다 큰 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "puzzle_letters = nltk.FreqDist('egivrvonl')\n",
    "obligatory = 'r'\n",
    "wordlist = nltk.corpus.words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glover',\n",
       " 'gorlin',\n",
       " 'govern',\n",
       " 'grovel',\n",
       " 'ignore',\n",
       " 'involver',\n",
       " 'lienor',\n",
       " 'linger',\n",
       " 'longer',\n",
       " 'lovering',\n",
       " 'noiler',\n",
       " 'overling',\n",
       " 'region',\n",
       " 'renvoi',\n",
       " 'revolving',\n",
       " 'ringle',\n",
       " 'roving',\n",
       " 'violer',\n",
       " 'virole']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wordlist if len(w) >= 6\n",
    "                        and obligatory in w\n",
    "                        and nltk.FreqDist(w) <= puzzle_letters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'e': 1, 'g': 1, 'i': 1, 'l': 1, 'n': 1, 'o': 1, 'r': 1, 'v': 2})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist('egivrvonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'e': 1, 'g': 1, 'l': 1, 'o': 1, 'r': 1, 'v': 1})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist('glover')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Pronouncing Dictionary\n",
    "\n",
    "좀더 풍부한 lexical resource 는 단어들과 어떤 property를 가지고 있는 테이블로 구성되어 있다. NLTK 는 CMU Pronouncing Dictionary 를 포함하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133737"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries = nltk.corpus.cmudict.entries()\n",
    "len(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('aamodt', ['AA1', 'M', 'AH0', 'T'])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.cmudict.entries()[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각각의 단어들은 phonetic codes(발음기호)의 리스트를 제공한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('explorer', ['IH0', 'K', 'S', 'P', 'L', 'AO1', 'R', 'ER0']),\n",
       " ('explorers', ['IH0', 'K', 'S', 'P', 'L', 'AO1', 'R', 'ER0', 'Z']),\n",
       " ('explores', ['IH0', 'K', 'S', 'P', 'L', 'AO1', 'R', 'Z']),\n",
       " ('exploring', ['IH0', 'K', 'S', 'P', 'L', 'AO1', 'R', 'IH0', 'NG']),\n",
       " ('explosion', ['IH0', 'K', 'S', 'P', 'L', 'OW1', 'ZH', 'AH0', 'N']),\n",
       " ('explosions', ['IH0', 'K', 'S', 'P', 'L', 'OW1', 'ZH', 'AH0', 'N', 'Z']),\n",
       " ('explosive', ['IH0', 'K', 'S', 'P', 'L', 'OW1', 'S', 'IH0', 'V']),\n",
       " ('explosively',\n",
       "  ['EH2', 'K', 'S', 'P', 'L', 'OW1', 'S', 'IH0', 'V', 'L', 'IY0'])]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries[39943:39951]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P,T 의 발음을 포함하는 단어들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pait EY1\n",
      "pat AE1\n",
      "pate EY1\n",
      "patt AE1\n",
      "peart ER1\n",
      "peat IY1\n",
      "peet IY1\n",
      "peete IY1\n",
      "pert ER1\n",
      "pet EH1\n",
      "pete IY1\n",
      "pett EH1\n",
      "piet IY1\n",
      "piette IY1\n",
      "pit IH1\n",
      "pitt IH1\n",
      "pot AA1\n",
      "pote OW1\n",
      "pott AA1\n",
      "pout AW1\n",
      "puett UW1\n",
      "purt ER1\n",
      "put UH1\n",
      "putt AH1\n"
     ]
    }
   ],
   "source": [
    "for word, pron in entries:\n",
    "    if len(pron) == 3:\n",
    "        ph1, ph2, ph3 = pron\n",
    "        if ph1 == 'P' and ph3 == 'T':\n",
    "            print (word, ph2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 끝의 발음이 NICK인 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"atlantic's\",\n",
       " 'audiotronics',\n",
       " 'avionics',\n",
       " 'beatniks',\n",
       " 'calisthenics',\n",
       " 'centronics',\n",
       " 'chamonix',\n",
       " 'chetniks',\n",
       " \"clinic's\",\n",
       " 'clinics',\n",
       " 'conics',\n",
       " 'conics',\n",
       " 'cryogenics',\n",
       " 'cynics',\n",
       " 'diasonics',\n",
       " \"dominic's\",\n",
       " 'ebonics',\n",
       " 'electronics',\n",
       " \"electronics'\",\n",
       " \"endotronics'\"]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syllable = ['N', 'IH0', 'K', 'S']\n",
    "[word for word, pron in entries if pron[-4:] == syllable][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dict 형식으로받아 올 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adduct': [['AE1', 'D', 'AH0', 'K', 'T']],\n",
       " 'basi': [['B', 'AA1', 'S', 'IY0'], ['B', 'EY1', 'S', 'IY0']],\n",
       " 'damping': [['D', 'AE1', 'M', 'P', 'IH0', 'NG']],\n",
       " 'gadbois': [['G', 'AE1', 'D', 'B', 'W', 'AA2']],\n",
       " 'girlfriend': [['G', 'ER1', 'L', 'F', 'R', 'EH2', 'N', 'D']],\n",
       " 'hey': [['HH', 'EY1']],\n",
       " 'laredo': [['L', 'ER0', 'EY1', 'D', 'OW0']],\n",
       " 'mcspadden': [['M', 'AH0', 'K', 'S', 'P', 'AE1', 'D', 'AH0', 'N']],\n",
       " 'pension': [['P', 'EH1', 'N', 'SH', 'AH0', 'N']],\n",
       " \"showboat's\": [['SH', 'OW1', 'B', 'OW2', 'T', 'S']]}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prondict = nltk.corpus.cmudict.dict()\n",
    "dict(list(prondict.items())[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['P', 'EH1', 'N', 'SH', 'AH0', 'N']]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prondict['pension']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['N', 'AE1', 'CH', 'ER0', 'AH0', 'L'],\n",
       " ['N', 'AE1', 'CH', 'R', 'AH0', 'L'],\n",
       " ['L', 'AE1', 'NG', 'G', 'W', 'AH0', 'JH'],\n",
       " ['L', 'AE1', 'NG', 'G', 'W', 'IH0', 'JH'],\n",
       " ['P', 'R', 'AA1', 'S', 'EH0', 'S', 'IH0', 'NG']]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['natural', 'language', 'processing']\n",
    "[ph for w in text for ph in prondict[w]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Wordlists\n",
    "\n",
    "다른 테이블로 된 lexicon(어휘) 는 <strong>comparative wordlist</strong> 이다. NLTK는 Swadesh wordlists corpus 를 포함하며 이는 200개의 평범한 단어로 구성되어 있다. 언어는 ISO 639 two-letter code 로 구분된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be',\n",
       " 'bg',\n",
       " 'bs',\n",
       " 'ca',\n",
       " 'cs',\n",
       " 'cu',\n",
       " 'de',\n",
       " 'en',\n",
       " 'es',\n",
       " 'fr',\n",
       " 'hr',\n",
       " 'it',\n",
       " 'la',\n",
       " 'mk',\n",
       " 'nl',\n",
       " 'pl',\n",
       " 'pt',\n",
       " 'ro',\n",
       " 'ru',\n",
       " 'sk',\n",
       " 'sl',\n",
       " 'sr',\n",
       " 'sw',\n",
       " 'uk']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import swadesh\n",
    "swadesh.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'you (singular), thou',\n",
       " 'he',\n",
       " 'we',\n",
       " 'you (plural)',\n",
       " 'they',\n",
       " 'this',\n",
       " 'that',\n",
       " 'here',\n",
       " 'there',\n",
       " 'who',\n",
       " 'what',\n",
       " 'where',\n",
       " 'when',\n",
       " 'how',\n",
       " 'not',\n",
       " 'all',\n",
       " 'many',\n",
       " 'some',\n",
       " 'few']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swadesh.words('en')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('я',\n",
       "  'аз',\n",
       "  'ja',\n",
       "  'jo',\n",
       "  'já',\n",
       "  'азъ',\n",
       "  'ich',\n",
       "  'I',\n",
       "  'yo',\n",
       "  'je',\n",
       "  'ja',\n",
       "  'io',\n",
       "  'ego',\n",
       "  'јас',\n",
       "  'ik',\n",
       "  'ja',\n",
       "  'eu',\n",
       "  'eu',\n",
       "  'я',\n",
       "  'ja',\n",
       "  'jaz',\n",
       "  'ја',\n",
       "  'jag',\n",
       "  'я'),\n",
       " ('ты',\n",
       "  'ти',\n",
       "  'ti',\n",
       "  'tu',\n",
       "  'ty',\n",
       "  'тъі',\n",
       "  'du, Sie',\n",
       "  'you (singular), thou',\n",
       "  'tú, usted',\n",
       "  'tu, vous',\n",
       "  'ti',\n",
       "  'tu, Lei',\n",
       "  'tu',\n",
       "  'ти',\n",
       "  'jij, je, U',\n",
       "  'ty',\n",
       "  'tu, você',\n",
       "  'tu',\n",
       "  'ты',\n",
       "  'ty',\n",
       "  'ti',\n",
       "  'ти',\n",
       "  'du',\n",
       "  'ти')]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swadesh.entries()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('je', 'I'),\n",
       " ('tu, vous', 'you (singular), thou'),\n",
       " ('il', 'he'),\n",
       " ('nous', 'we'),\n",
       " ('vous', 'you (plural)'),\n",
       " ('ils, elles', 'they'),\n",
       " ('ceci', 'this'),\n",
       " ('cela', 'that'),\n",
       " ('ici', 'here'),\n",
       " ('là', 'there'),\n",
       " ('qui', 'who'),\n",
       " ('quoi', 'what'),\n",
       " ('où', 'where'),\n",
       " ('quand', 'when'),\n",
       " ('comment', 'how'),\n",
       " ('ne...pas', 'not'),\n",
       " ('tout', 'all'),\n",
       " ('plusieurs', 'many'),\n",
       " ('quelques', 'some'),\n",
       " ('peu', 'few')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr2en = swadesh.entries(['fr', 'en'])\n",
    "fr2en[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ceci': 'this',\n",
       " 'chasser': 'hunt',\n",
       " 'cou': 'neck',\n",
       " 'court': 'short',\n",
       " 'dans': 'in',\n",
       " 'genou': 'knee',\n",
       " 'herbe': 'grass',\n",
       " 'mince': 'thin',\n",
       " 'mordre': 'bite',\n",
       " 'poisson': 'fish',\n",
       " 'poussière': 'dust',\n",
       " \"s'asseoir\": 'sit',\n",
       " 'sang': 'blood',\n",
       " 'se lever': 'stand',\n",
       " 'soleil': 'sun',\n",
       " 'souffler': 'blow',\n",
       " 'tenir': 'hold',\n",
       " 'tomber': 'fall',\n",
       " 'tranchant, pointu, aigu': 'sharp',\n",
       " 'tuer': 'kill'}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate = dict(fr2en)\n",
    "dict(list(translate.items())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shoebox and Toolbox Lexicons\n",
    "\n",
    "아마 데이터를 처리하는데 가장 유명한 툴은 toolbox 일것이다. toolbox file은 entries의 집합으로 구성되어 있고 각각의 entry는 하나 혹은 더많은 fileid 로 만들어져 있다. 대부분의 fileids 는 optional 하거나 반복 가능하고 이것의 의미는 이런 lexical resource 는 테이블로 처리될 수 없다는 것이다\n",
    "\n",
    "### Rotokas 언어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'toolbox' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-511e07131a71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtoolbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'toolbox' is not defined"
     ]
    }
   ],
   "source": [
    "toolbox.entries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:64: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if 'order' in inspect.getargspec(np.copy)[0]:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import toolbox\n",
    "toolbox.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kaa',\n",
       "  [('ps', 'V'),\n",
       "   ('pt', 'A'),\n",
       "   ('ge', 'gag'),\n",
       "   ('tkp', 'nek i pas'),\n",
       "   ('dcsv', 'true'),\n",
       "   ('vx', '1'),\n",
       "   ('sc', '???'),\n",
       "   ('dt', '29/Oct/2005'),\n",
       "   ('ex', 'Apoka ira kaaroi aioa-ia reoreopaoro.'),\n",
       "   ('xp', 'Kaikai i pas long nek bilong Apoka bikos em i kaikai na toktok.'),\n",
       "   ('xe', 'Apoka is gagging from food while talking.')]),\n",
       " ('kaa',\n",
       "  [('ps', 'V'),\n",
       "   ('pt', 'B'),\n",
       "   ('ge', 'strangle'),\n",
       "   ('tkp', 'pasim nek'),\n",
       "   ('arg', 'O'),\n",
       "   ('vx', '2'),\n",
       "   ('dt', '07/Oct/2006'),\n",
       "   ('ex', 'Rera rauroro rera kaarevoi.'),\n",
       "   ('xp', 'Em i holim pas em na nekim em.'),\n",
       "   ('xe', 'He is holding him and strangling him.'),\n",
       "   ('ex', 'Iroiro-ia oirato okoearo kaaivoi uvare rirovira kaureoparoveira.'),\n",
       "   ('xp', 'Ol i pasim nek bilong man long rop bikos em i save bikhet tumas.'),\n",
       "   ('xe',\n",
       "    \"They strangled the man's neck with rope because he was very stubborn and arrogant.\"),\n",
       "   ('ex',\n",
       "    'Oirato okoearo kaaivoi iroiro-ia. Uva viapau uvuiparoi ra vovouparo uva kopiiroi.'),\n",
       "   ('xp',\n",
       "    'Ol i pasim nek bilong man long rop. Olsem na em i no pulim win olsem na em i dai.'),\n",
       "   ('xe',\n",
       "    \"They strangled the man's neck with a rope. And he couldn't breathe and he died.\")])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toolbox.entries('rotokas.dic')[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 WordNet\n",
    "\n",
    "<strong>WordNet</strong> 은 영단어의들을 모아놓은 사전이다. NLTK 는 English WordNeT을 포함하고 있고 155,287개의 단어와 117,659개의 동의어들로 이루어져 있다.\n",
    "\n",
    "# Senses and Synonyms\n",
    "\n",
    "1. Benz is credited with the invention of the motorcar.\n",
    "2. Benz is credited with the invention of the automobile.\n",
    "\n",
    "1번 문장의 motorcar 라는 단어가 authomobile로 바꿀 수 있다면 두 단어는 동의어이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('motorcar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "motocar 는 car.n.01 이라는 단 하나를 가지고 있는데 이런 car.n.01 같은 entity 를 <strong>synset</strong> 이라고 한다 동의어들의 집합이다.\n",
    "\n",
    "lemma - word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('car.n.01')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he needs a car to get to work']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 동의어가 주어졌을때 그 단어들\n",
    "2. 특정 단어\n",
    "3. 특정 단어의 동의어\n",
    "4. 단어의 name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('car.n.01.car'),\n",
       " Lemma('car.n.01.auto'),\n",
       " Lemma('car.n.01.automobile'),\n",
       " Lemma('car.n.01.machine'),\n",
       " Lemma('car.n.01.motorcar')]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lemma('car.n.01.automobile')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('car.n.01.automobile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('car.n.01')"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('car.n.01.automobile').synset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'automobile'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('car.n.01.automobile').name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01'),\n",
       " Synset('car.n.02'),\n",
       " Synset('car.n.03'),\n",
       " Synset('car.n.04'),\n",
       " Synset('cable_car.n.01')]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'auto', 'automobile', 'machine', 'motorcar']\n",
      "['car', 'railcar', 'railway_car', 'railroad_car']\n",
      "['car', 'gondola']\n",
      "['car', 'elevator_car']\n",
      "['cable_car', 'car']\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets('car'):\n",
    "    print (synset.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('car.n.01.car'),\n",
       " Lemma('car.n.02.car'),\n",
       " Lemma('car.n.03.car'),\n",
       " Lemma('car.n.04.car'),\n",
       " Lemma('cable_car.n.01.car')]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemmas('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
      "\n",
      "a wheeled vehicle adapted to the rails of railroad\n",
      "\n",
      "the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant\n",
      "\n",
      "where passengers ride up and down\n",
      "\n",
      "a conveyance for passengers or freight on a cable railway\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets('car'):\n",
    "    print(synset.definition()+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The WordNet Hierarchy\n",
    "\n",
    "WordNet synset 은 추상적 개념으로 구성되 있다. 그리고 그들은 항상 영단어와 일치하지는 않는다. 이 개념은 Hierarchy 하게 연결되어 있다. Entity, State, Event 같은 개념은 매우 일반적이다. 이런 일반적인 개념들을 <strong>unique beginners </strong> or <strong>root system </strong> 이라고 한다.\n",
    "\n",
    "<img src=\"picture/캡처5.JPG\" width=600 />\n",
    "\n",
    "WordNet은 이런 개념들을 쉽게 찾아 볼 수 있게 한다. 예를 들면 motocar 라는 개념이 주어 졌을때 <strong>hyponyms</strong>(하위어) 들을 살펴볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('ambulance.n.01'),\n",
       " Synset('beach_wagon.n.01'),\n",
       " Synset('bus.n.04'),\n",
       " Synset('cab.n.03'),\n",
       " Synset('compact.n.03'),\n",
       " Synset('convertible.n.01'),\n",
       " Synset('coupe.n.01'),\n",
       " Synset('cruiser.n.01'),\n",
       " Synset('electric.n.01'),\n",
       " Synset('gas_guzzler.n.01'),\n",
       " Synset('hardtop.n.01'),\n",
       " Synset('hatchback.n.01'),\n",
       " Synset('horseless_carriage.n.01'),\n",
       " Synset('hot_rod.n.01'),\n",
       " Synset('jeep.n.01'),\n",
       " Synset('limousine.n.01'),\n",
       " Synset('loaner.n.02'),\n",
       " Synset('minicar.n.01'),\n",
       " Synset('minivan.n.01'),\n",
       " Synset('model_t.n.01'),\n",
       " Synset('pace_car.n.01'),\n",
       " Synset('racer.n.02'),\n",
       " Synset('roadster.n.01'),\n",
       " Synset('sedan.n.01'),\n",
       " Synset('sport_utility.n.01'),\n",
       " Synset('sports_car.n.01'),\n",
       " Synset('stanley_steamer.n.01'),\n",
       " Synset('stock_car.n.01'),\n",
       " Synset('subcompact.n.01'),\n",
       " Synset('touring_car.n.01'),\n",
       " Synset('used-car.n.01')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motorcar = wn.synset('car.n.01')\n",
    "types_of_motorcar = motorcar.hyponyms()\n",
    "types_of_motorcar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Model_T',\n",
       " 'S.U.V.',\n",
       " 'SUV',\n",
       " 'Stanley_Steamer',\n",
       " 'ambulance',\n",
       " 'beach_waggon',\n",
       " 'beach_wagon',\n",
       " 'bus',\n",
       " 'cab',\n",
       " 'compact',\n",
       " 'compact_car',\n",
       " 'convertible',\n",
       " 'coupe',\n",
       " 'cruiser',\n",
       " 'electric',\n",
       " 'electric_automobile',\n",
       " 'electric_car',\n",
       " 'estate_car',\n",
       " 'gas_guzzler',\n",
       " 'hack']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([lemma.name() for synset in types_of_motorcar for lemma in synset.lemmas()])[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 단어들은 여러 경로를 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('motor_vehicle.n.01')]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motorcar.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('entity.n.01'),\n",
       "  Synset('physical_entity.n.01'),\n",
       "  Synset('object.n.01'),\n",
       "  Synset('whole.n.02'),\n",
       "  Synset('artifact.n.01'),\n",
       "  Synset('instrumentality.n.03'),\n",
       "  Synset('container.n.01'),\n",
       "  Synset('wheeled_vehicle.n.01'),\n",
       "  Synset('self-propelled_vehicle.n.01'),\n",
       "  Synset('motor_vehicle.n.01'),\n",
       "  Synset('car.n.01')],\n",
       " [Synset('entity.n.01'),\n",
       "  Synset('physical_entity.n.01'),\n",
       "  Synset('object.n.01'),\n",
       "  Synset('whole.n.02'),\n",
       "  Synset('artifact.n.01'),\n",
       "  Synset('instrumentality.n.03'),\n",
       "  Synset('conveyance.n.03'),\n",
       "  Synset('vehicle.n.01'),\n",
       "  Synset('wheeled_vehicle.n.01'),\n",
       "  Synset('self-propelled_vehicle.n.01'),\n",
       "  Synset('motor_vehicle.n.01'),\n",
       "  Synset('car.n.01')]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = motorcar.hypernym_paths()\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<bound method Synset.name of Synset('entity.n.01')>,\n",
       " <bound method Synset.name of Synset('physical_entity.n.01')>,\n",
       " <bound method Synset.name of Synset('object.n.01')>,\n",
       " <bound method Synset.name of Synset('whole.n.02')>,\n",
       " <bound method Synset.name of Synset('artifact.n.01')>,\n",
       " <bound method Synset.name of Synset('instrumentality.n.03')>,\n",
       " <bound method Synset.name of Synset('container.n.01')>,\n",
       " <bound method Synset.name of Synset('wheeled_vehicle.n.01')>,\n",
       " <bound method Synset.name of Synset('self-propelled_vehicle.n.01')>,\n",
       " <bound method Synset.name of Synset('motor_vehicle.n.01')>,\n",
       " <bound method Synset.name of Synset('car.n.01')>]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[synset.name for synset in paths[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<bound method Synset.name of Synset('entity.n.01')>,\n",
       " <bound method Synset.name of Synset('physical_entity.n.01')>,\n",
       " <bound method Synset.name of Synset('object.n.01')>,\n",
       " <bound method Synset.name of Synset('whole.n.02')>,\n",
       " <bound method Synset.name of Synset('artifact.n.01')>,\n",
       " <bound method Synset.name of Synset('instrumentality.n.03')>,\n",
       " <bound method Synset.name of Synset('conveyance.n.03')>,\n",
       " <bound method Synset.name of Synset('vehicle.n.01')>,\n",
       " <bound method Synset.name of Synset('wheeled_vehicle.n.01')>,\n",
       " <bound method Synset.name of Synset('self-propelled_vehicle.n.01')>,\n",
       " <bound method Synset.name of Synset('motor_vehicle.n.01')>,\n",
       " <bound method Synset.name of Synset('car.n.01')>]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[synset.name for synset in paths[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motorcar.root_hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "GetoptError",
     "evalue": "option -f not recognized",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mGetoptError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-203-67b650ab6039>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwordnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\nltk\\app\\wordnet_app.py\u001b[0m in \u001b[0;36mapp\u001b[1;34m()\u001b[0m\n\u001b[0;32m    943\u001b[0m     \u001b[1;31m# Parse and interpret options.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m     (opts, _) = getopt.getopt(argv[1:], \"l:p:sh\",\n\u001b[1;32m--> 945\u001b[1;33m                               [\"logfile=\", \"port=\", \"server-mode\", \"help\"])\n\u001b[0m\u001b[0;32m    946\u001b[0m     \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m     \u001b[0mserver_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\getopt.py\u001b[0m in \u001b[0;36mgetopt\u001b[1;34m(args, shortopts, longopts)\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[0mopts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdo_longs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlongopts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mopts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdo_shorts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshortopts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\getopt.py\u001b[0m in \u001b[0;36mdo_shorts\u001b[1;34m(opts, optstring, shortopts, args)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0moptstring\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptstring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptstring\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptstring\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mshort_has_arg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshortopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0moptstring\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\getopt.py\u001b[0m in \u001b[0;36mshort_has_arg\u001b[1;34m(opt, shortopts)\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mshortopts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m':'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mshortopts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mGetoptError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'option -%s not recognized'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mGetoptError\u001b[0m: option -f not recognized"
     ]
    }
   ],
   "source": [
    "#nltk.app.wordnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Lexical Relations \n",
    "\n",
    "Hypernyms(상위어) hyponyms(하위어) 들은 <strong>lexical relations</strong> 이라고 불린다. 이 둘 관계는 \"is-a\" 위계를 위아래로 갔다왔다 한다. WordNet 을 탐험하는 다른 ㅈㅇ요한 이유는 그들의 구소요소(meronyms) 혹은 그들이 포함하고 있는 하위어(holonyms). 예를 들면 tree의 부분은 그것의 trunk(몸통),crown(꼭대기) 등등으로 이루어져 있는데 these are the part_meronyms(). The substance\n",
    "a tree is made of includes heartwood(심재) and sapwood(변재 (나무껍질 바로 안쪽, 통나무의 겉 부분에 해당하는 희고 무른 부분)), i.e., the substance_meronyms(). A\n",
    "collection of trees forms a forest, i.e., the member_holonyms():\n",
    "\n",
    "- ### part_meronyms 부분 부분어\n",
    "- ### substance_meronyms 실체 부분어\n",
    "- ### member_holonyms 멤버 하위어?\n",
    "- ### entailments 수반어\n",
    "- ### antonyms 반의어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('burl.n.02'),\n",
       " Synset('crown.n.07'),\n",
       " Synset('limb.n.02'),\n",
       " Synset('stump.n.01'),\n",
       " Synset('trunk.n.01')]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('tree.n.01').part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('library.n.01'),\n",
       " Synset('loft.n.02'),\n",
       " Synset('porch.n.01'),\n",
       " Synset('study.n.05')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('house.n.01').part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('human.n.01').substance_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('forest.n.01')]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('tree.n.01').member_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('step.v.01')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('walk.v.01').entailments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('chew.v.01'), Synset('swallow.v.01')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('eat.v.01').entailments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.synset('tease.v.03').entailments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.lemma('supply.n.02.supply').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('linger.v.04.linger')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('rush.v.01.rush').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.lemma('horizontal.a.01.horizontal').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.lemma('staccato.r.01.staccato').antonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Similarity\n",
    "\n",
    "우리는 synsets 이 복잡한 lexical relations 에 의해 연결되 있는걸 보았다. 특정한 synset이 주어졌을대 우리는 관련된 의미를 찾기위해 Wordnet을 이용할 수 있다. 이런단어들이 의미적으로 관련되어 있다는 것은 text의 집합을 indexing 할대 유용하다. limousine(리무진)이라는 단어를 포함하는 문서를 vehicle 이라는 term으로 검색 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "right = wn.synset('right_whale.n.01')\n",
    "orca = wn.synset('orca.n.01')\n",
    "minke = wn.synset('minke_whale.n.01')\n",
    "tortoise = wn.synset('tortoise.n.01')\n",
    "novel = wn.synset('novel.n.01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 공통의 하위어가 뭔지를 찾는예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('baleen_whale.n.01')] [Synset('whale.n.02')] [Synset('vertebrate.n.01')] [Synset('entity.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(right.lowest_common_hypernyms(minke),\n",
    "    right.lowest_common_hypernyms(orca),\n",
    "    right.lowest_common_hypernyms(tortoise),\n",
    "    right.lowest_common_hypernyms(novel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각의 synset 들은 하나 혹은 더 많은 하위어 들을 가지고 있다. 그것의 root hypernyms 들은 entity, event 등등일 것이다. 만약 두 synset이 어떠한 하위어들을 공유한다면 그들은 가까운 관계라는 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 13 8 0\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset('baleen_whale.n.01').min_depth(),\n",
    "    wn.synset('whale.n.02').min_depth(),\n",
    "    wn.synset('vertebrate.n.01').min_depth(),\n",
    "    wn.synset('entity.n.01').min_depth())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path_similarity 는 두개념과의 가장 가까운 하위어에 따라서 0~1 사이의 스코어를 매긴다 (-1 은 두 개념이 공유하는 하위어가 없는 것이다) 어떤 synset의 그자신과 비교는 1을 리턴한다 . \n",
    "\n",
    "밑의 예제에서 right 고래와 minke,orca,tortoise,novel 사이의 유사도는 많은것을 의미하지는 않지만 점점 떨어져 가는것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25 0.16666666666666666 0.07692307692307693 0.043478260869565216\n"
     ]
    }
   ],
   "source": [
    "print(right.path_similarity(minke),\n",
    "    right.path_similarity(orca),\n",
    "    right.path_similarity(tortoise),\n",
    "    right.path_similarity(novel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
