{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BasicRNNCell\n",
    "#### TensorFlow 0.9 implementation based on hunkim's tutorial\n",
    "https://hunkim.github.io/ml/\n",
    "\n",
    "https://www.youtube.com/watch?v=A8wJYfDUYCk&feature=youtu.be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_rdic = ['h', 'e', 'l', 'o'] # id -> char\n",
    "char_dic = {w : i for i, w in enumerate(char_rdic)} # char -> id\n",
    "ground_truth = [char_dic[c] for c in 'hello']\n",
    "print(ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# x값 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = tf.one_hot(ground_truth[:-1], 4, 1.0, 0.0, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rnn_size , batch_size 설정\n",
    "\n",
    "rnn_size 는 아웃풋의 갯수  \n",
    "batch_size는 batch 사이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_size = len(char_dic) # 4\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델생성과 첫번째 상태 초기화\n",
    "\n",
    "https://www.quora.com/Why-using-sigmoid-and-tanh-as-the-activation-functions-in-LSTM-or-RNN-is-not-problematic-but-this-is-not-the-case-in-other-neural-nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units = rnn_size,\n",
    "                                       input_size = None, # deprecated at tensorflow 0.9\n",
    "                                       #activation = tanh,\n",
    "                                       )\n",
    "\n",
    "initial_state = rnn_cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# x값을 split\n",
    "\n",
    "x_split output \n",
    "\n",
    "```python\n",
    "[[1,0,0,0]] # h  \n",
    "[[0,1,0,0]] # e  \n",
    "[[0,0,1,0]] # l  \n",
    "[[0,0,0,1]] # l  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_split = tf.split(0, len(char_dic), x_data) # 가로축으로 4개로 split\n",
    "\n",
    "outputs, state = tf.nn.rnn(cell = rnn_cell, inputs = x_split, initial_state = initial_state)\n",
    "print outputs, '\\n'\n",
    "print state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTPUT을 RESHAPE\n",
    "\n",
    "\n",
    "['h', 'e', 'l', 'o']  \n",
    "logits =  \n",
    "[[h일확률,e일확률,l일확률,o일확률],   첫번째 아웃풋  \n",
    "        [h일확률,e일확률,l일확률,o일확률],   두번째 아웃풋  \n",
    "        [h일확률,e일확률,l일확률,o일확률],  세번째 아웃풋  \n",
    "        [h일확률,e일확률,l일확률,o일확률]]       네번째 아웃풋  \n",
    "        \n",
    "왜 concat 하지? print outputs, '\\n' 이결과 [1,4,4] 의 형태라서!!   \n",
    "If values[i].shape = [D0, D1, ... Dconcat_dim(i), ...Dn], the concatenated result has shape\n",
    " [D0, D1, ... Rconcat_dim, ...Dn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tf.concat = Concatenates tensors along one dimension.\n",
    "\n",
    "logits = tf.reshape(tf.concat(1, outputs),  # shape = 1 x 16\n",
    "                    [-1, rnn_size])  # shape = 4 x 4\n",
    "\n",
    "#y data\n",
    "\n",
    "targets = tf.reshape(ground_truth[1:], [-1]) # [[e],[l],[l],[o]]\n",
    "\n",
    "# weight \n",
    "weights = tf.ones([len(char_dic) * batch_size])\n",
    " \"\"\"Most basic RNN: output = new_state = activation(W * input + U * state + B).\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cost, loss function 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.nn.seq2seq.sequence_loss_by_example([logits], [targets], [weights])\n",
    "cost = tf.reduce_sum(loss) / batch_size\n",
    "train_op = tf.train.RMSPropOptimizer(0.01, 0.9).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for i in range(100):\n",
    "        sess.run(train_op)\n",
    "        result = sess.run(tf.argmax(logits, 1))\n",
    "        print(result, [char_rdic[t] for t in result])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
