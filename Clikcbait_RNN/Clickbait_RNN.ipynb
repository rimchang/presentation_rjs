{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Generating Clickbait With Recurrent Neural Networks\n",
    "\n",
    "<img src=\"https://larseidnes.files.wordpress.com/2015/09/1204-fdrwarplansa.jpg?w=300&h=221\" width=300 />\n",
    "\n",
    "### Clikbait : 자극적인 제목으로 인터넷 유저들의 클릭을 유도하여 조회수를 높이는 쓰레기 기사나 광고(신조어)\n",
    "\n",
    "1941 에 쓰여진 “F.D.R.’s War Plans!”이라는 헤드라인이 요즘에 다시 쓰인다면 “21 War Plans F.D.R. Does Not Want You To Know About. Number 6 may shock you!”라고 쓰일 만 한다. 현대의 기자들은 자극적인 기사를 쓰는데에 온 힘을 다한다. 그리고 이런 자극적인 헤드라인들은 매우 정형화되고 독창적이지 않다. 우리가 이런 자극적인 기사들을 자동적으로 생성한다면 매우 도움이 되지 않을까???라는 생각을 하게 된다.\n",
    "\n",
    "이런 자극적인 헤드라인들이 정형화 되어 있다면 우리는 RNN을 통해서 자동적으로 생성해 낼 수 있다.\n",
    "\n",
    "표준적인 Neural network 는 어떤 인풋에 따라 아웃풋을 도출하는 함수를 배우는 머신 러닝이다. 이때 충분한 샘플이 필요하다. 최근들어 사람들이 deep neural net을 학습시키는 방법 즉 딥러닝을 많이 연구하면서 NN 이 강력해 졌다. 이런 NN 을 사용한 아이디어 중의 하나가 RNN 인데 이것은 이전 State 가 다음 State 에 영향을 미치는 NN의 형태이다.\n",
    "\n",
    "## 일반적인 NN\n",
    "<img src=\"http://imgur.com/yE88Ryt.png\" width=500 />\n",
    "\n",
    "## RNN\n",
    "<img src=\"https://larseidnes.files.wordpress.com/2015/09/rnn-unrolled2.png\" width=500 />\n",
    "\n",
    "> RNN은 순환적인 구조를 뛴다. 오른쪽의 예제는 3step을 거친 예제이고 시간의 흐름에 따라 NN처럼 학습시킬 수 있다.\n",
    "\n",
    "우리는 RNN을 단어의 연속으로 볼 수 있고 RNN을 통해서 이전 단어를 통해 다음 단어를 예측한다. 만약 “Which Disney Character Are ( ) \" 라는 문장이 주어진다면 우리는 you 라는 단어가 Spreadsheet 보다 더 가능성이 높다고 생각 할 수 잇따. 만약 RNN을 통해 다음 단어가 나올 가능성을 예측 할 수 있다면 우리는 *language model* 을 얻을 수 있다. \n",
    "\n",
    ">language model : statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length m\n",
    "\n",
    "우리가 모델을 통해 다음 단어를 예측하고 그 다음 단어를 예측하는 것을 통해서 어떠한 길이를 가진 텍스트를 만들 수 있다. 학습의 과정 동안 RNN의 prediction error를 최소화 하고 바로 다음 단어가 나올 가능성을 최대화 한다. 그래서 RNN 은 Clicbait를 만들기에 적절하다\n",
    "\n",
    "여기서는  Andrej Karpathy의 char-rnn 을 기반으로 word-rnn 으로 확장 시켰다. word-by-word 예측은 더 많은 메모리가 필요하지만 단어의 스펠링을 배우지 않아도 된다는 것을 의미한다. 더 나은 결과를 위해 몇가지 변화를 주었는데. \n",
    "\n",
    "첫번째로 each input word was represented as a dense vector of numbers. 인풋 word 는 밀집행렬(중간중간 비어있는게 없는 행렬 sparse vector의 반댓말) 이러한 밀집행렬을 사용함으로써 이산적이기보단 연속적인 이점을 가져 network가 better mistakes을 할 수 있게 한다. \n",
    "\n",
    "두번째로 Adam optimizer를 사용했다.\n",
    "\n",
    "세번째로 the word vectors went through a particular training rigmarole(복잡한 절차) \n",
    "두단계의 pretraining을 거친 후 마지막 네트워크에 학습을 시켰다.\n",
    "\n",
    "<img src=\"https://larseidnes.files.wordpress.com/2015/09/architecture.png\" width=500 />\n",
    "\n",
    "http://deeplearning4j.org/kr-lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Neat Trick Every 90s Connectionist Will Know\n",
    "\n",
    "전통적인 NN이 간단한 non-linear function(ex tanh) 같은 간단한 유닛을 쌓아서 만들지만 우리는 더 복잡한 LSTM unit을 사용했다. LSTM units은 read, write, reset 의 operations으로 구성되어 있다. 이런 opearatinos들은 미분 가능해서 언제 데이터를 *remember* 하고 언제 *throw it away* 할지를 학습 할 수 있다.\n",
    "\n",
    "clickbait를 만들기 위해서 우린 RNN에 2000000개의 헤드라인을 학습시켰다\n",
    "\n",
    "GTX980 GPU을 통한 몇일동안의 학습결과 어떠한 결과를 얻을 수 있었다.\n",
    "\n",
    "초기의 학습결과에서는 일관성 없는 단어들을 생성해 냈다. \n",
    "40000 헤드라인을 학습한 결과이다\n",
    "\n",
    ">2 0 Million 9 0 1 3 Say Hours To Stars The Kids For From Internet  \n",
    "Adobe ‘ s Saving New Japan  \n",
    "Real Walk Join Their Back For Plane To French Sarah York  \n",
    "State 7  \n",
    "Dr 5 Gameplay : Oscars Strong As The Dead  \n",
    "Economic Lessons To Actress To Ex – Takes A App  \n",
    "You ‘ s Schools ‘ : A Improve Story  \n",
    "\n",
    "그러나 데이터들을 많이 반복해서 학습한 결과 놀라운 성능향상이 있었다.\n",
    "이것은 학습을 완료한 첫번째 아웃풋이다(EPOCH 한번 돌렸다는것인듯?)\n",
    "\n",
    ">John McCain Warns Supreme Court To Stand Up For Birth Control Reform  \n",
    "Earth Defense Force : Record Olympic Fans  \n",
    "Kate Middleton , Prince William & Prince George Leave Kate For The Queen  \n",
    "The Most Creative Part Of U . S . History  \n",
    "Biden Responds To Hillary Clinton ‘ s Speech  \n",
    "The Children Of Free Speech  \n",
    "Adam Smith And Jennifer Lawrence ( And Tiger Woods ” Break The Ice Ball , For This Tornado )  \n",
    "\n",
    "이 결과는 매우 그럴듯한 헤드라인을 만들어 내서 놀라웠는데 대부분이 문법적으로 맞고 많은 헤드라인이 말이 되었다.\n",
    "\n",
    " 나는 의심이 들었는데 이 모델이 단지 데이터셋에서 headline을 기억하고 있는 것이 아닌가 하는 의심이 들었다. 하지만 “Mary J. Williams On Coming Out As A Woman”이 문장을 보고 그런 경우가 아니라고 생각을 했다. 이 문장의 *“Coming Out As A Woman”*의 부분은 dataset 중에  “Former Marine Chronicles Journey *Coming Out As A Trans Woman* On YouTube”이라는 부분만 비슷했고 “Mary J. Williams”라는 이름은 dataset에 있지도 않았다. 이 모델은 “Mary J. Williams”이 그럴듯한 이름이고 이것이 보통 여자의 이름이라는 것을 학습한것이다.\n",
    " \n",
    " \n",
    " 다른 좋은 예는 “Romney Camp: ‘I Think You Are A Bad President\"이였는데 이 문장은 실제로 다른 사이트에서 헤드라인으로 쓰였다. 심지어 데이터셋에 있지도 않고 비슷한 것도 없었다. “Romney Camp”라는 단어만 17번 dataset에 존재 했고  “Bad President” 라는 단어가 한번 나왔다. 우리가 만든 모델이 어떠한 세상의 지식을 학습한 것처럼 보였다.\n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kim Kardashian Is Married With A Baby In New Mexico\n",
    "\n",
    "이제 이런  semantic associations(의미적 연합) 을 시험해 보자 RNN 에 시작할 문장을 주고 그 문장을 통해 결과를 내봤다. 매우 놀라운 결과를 보였는데 “Barack Obama Says” and “Kim Kardashian Says”라는 Seed를 준 결과이다.\n",
    "\n",
    "Here are the 10 first completions of “Barack Obama Says”:\n",
    "\n",
    ">Barack Obama Says It’s Wrong To Talk About Iraq  \n",
    "Barack Obama Says He’s Like ‘A Single Mother’ And ‘Over The Top’  \n",
    "Barack Obama Says He Is “The First Threat Gay Woman In The World”  \n",
    "Barack Obama Says About Ohio Law  \n",
    "Barack Obama Says He Is Wrong  \n",
    "Barack Obama Says He Will Get The American Idol  \n",
    "Barack Obama Says Himself Are “Doing Well Around The World”  \n",
    "Barack Obama Says As He Leaves Politics With His Wife  \n",
    "Barack Obama Says He Did 48 Things Over \n",
    "Barack Obama Says GOP Needs To Be Key To New Immigration Policy  \n",
    "\n",
    "And here are the 10 first completions of “Kim Kardashian Says”:\n",
    "\n",
    ">Kim Kardashian Says She Wants To Sign Again  \n",
    "Kim Kardashian Says ‘Idea’ To Her Mean Baby!  \n",
    "Kim Kardashian Says North West Is Even More ‘Important’  \n",
    "Kim Kardashian Says She Would Love Kanye  \n",
    "Kim Kardashian Says She’s A Hero  \n",
    "Kim Kardashian Says She Looks Fake  \n",
    "Kim Kardashian Says It Was Over Before They Call Her  \n",
    "Kim Kardashian Says Her Book Used To Lose Her Cool  \n",
    "Kim Kardashian Says She’s Married With A Baby In New Mexico  \n",
    "Kim Kardashian Says Kanye West Needs A Break From Her  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering\n",
    "\n",
    "RNN이 어떤 문장을 완성시키게 함으로써 우리는 효과적인 ask questions을 얻을 수 있다. . Ilya Sutskever and Geoff Hinton은 위키피디아 데이터를 가지고 Char RNN 을 학습시켜 “The meaning of life is”이라는 문장을 완성 시키게 했다 RNN은 \"human reproduction(생식)\" 이라는 답을 했다고 한다. 이는 얼추 말이 된다는 점에서 매우 재밌는 결과였다. 물론 생물학적 관점에서\n",
    "\n",
    "우리도 똑같은 것을 해봤다 “Life Is About”라는 문장을 완성시키도록 해봤다\n",
    "\n",
    ">Life Is About The Weather!  \n",
    "Life Is About The (Wild) Truth About Human-Rights  \n",
    "Life Is About The True Love Of Mr. Mom  \n",
    "Life Is About Where He Were Now  \n",
    "Life Is About Kids  \n",
    "Life Is About What It Takes If Being On The Spot Is Tough  \n",
    "Life Is About A Giant White House Close To A Body In These Red Carpet Looks From   Prince William’s Epic ‘Dinner With Johnny’\n",
    "Life Is About — Or Still Didn’t Know Me  \n",
    "Life Is About… An Eating Story  \n",
    "Life Is About The Truth Now  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network details\n",
    "\n",
    "초기의 RNN 은 2개의 recurrent layer를 가졌고 각각은 1200개의 LSTM 유닛을 포함한다. 각각의 단어는 200개의 dimensional word vector로 나타내졌고 네트워크에 학습되었다. 이런 word vectors 는 Wikipeda의 600백만개의 토큰을 학습시킨 GloVe vector를 사용해 초기화 시켰고 이것은 pretrain 효과가 있다. 이 GloVe vector 는 매우 큰 dataset 으로 학습되었기 때문에 우리의 단어에도 좋은 초기치로 사용된다. we can follow the gradient down into these word vectors and fine-tune the vector representations specifically for the task of generating clickbait, thus further improving the generalization accuracy of the complete model.\n",
    "\n",
    "It turns out that if we then take the word vectors learned from this model of 2 recurrent layers, and stick them in an architecture with 3 recurrent layers, and then freeze them, we get even better performance. Trying to backpropagate into the word vectors through the 3 recurrent layers turned out to actually hurt performance.\n",
    "\n",
    "<img src=\"https://larseidnes.files.wordpress.com/2015/09/graph1.png?w=1008\" width=500 />\n",
    "\n",
    "To summarize the word vector story: Initially, some good guys at Standford invented GloVe, ran it over 6 billion tokens, and got a bunch of vectors. We then took these vectors, stuck them under 2 recurrent LSTM layers, and optimized them for generating clickbait. Finally we froze the vectors, and put them in a 3 LSTM layer architecture.\n",
    "\n",
    "The network was trained with the Adam optimizer. I found this to be a Big Deal: It cut the training time almost in half, and found better optima, compared to using rmsprop with exponential decay. It’s possible that similar results could be obtained with rmsprop had I found a better learning and decay rate, but I’m very happy not having to do that tuning.\n",
    "\n",
    "<img src=\"https://larseidnes.files.wordpress.com/2015/09/graph2.png?w=1008\" width=500 />\n",
    "\n",
    ">### 임베딩 레이어 추가하기\n",
    "\n",
    ">Word2vec이나 GloVe와 같은 단어 임베딩을 추가하는 것은 모델의 성능을 향상시키기 위해 자주 사용되는 방법입니다. 각 단어의 one-hot 벡터 표현법과 달리 word2vec이나 GloVe에서 학습된 낮은 차원의 벡터 표현은 그 단어의 의미 정보를 담게 됩니다. 즉, 비슷한 단어는 비슷한 벡터 값을 갖게 됩니다. 이것들을 사용하는 것은 pre-training (딥러닝에서 한번에 학습이 어렵기 때문에 단계별로 네트워크의 일부분을 미리 학습해 두는 작업)처럼 생각할 수 있습니다. 간단히 말하자면, 이 과정을 통해 네트워크가 언어에 대한 정보를 (미리 어느정도는 학습되어 있어서) 학습해야 될 부분이 줄어들게 되는 것입니다. Pre-train 된 벡터들은 학습할 데이터가 많지 않을 때 특히 유용한데, 네트워크가 사전에 보지 못한 단어들에 대해서도 일반화가 가능해지기 때문입니다. 여기서는 pre-train 된 단어 벡터를 사용하지는 않았지만, 임베딩 레이어 (코드의 행렬 E)를 추가함으로써 미리 학습된 단어 벡터를 넣는 것이 쉽게 해 두었습니다. 임베딩 행렬은 결국 lookup table - i번째 열 벡터가 우리 단어장의 i번째 단어에 해당합니다 - 의 형태입니다. 행렬 E를 업데이트한다는 것은 단어 벡터 자신을 학습한다는 것인데, 이는 우리가 원하는 태스크 (와 데이터셋)에만 잘 동작하도록 맞춰지는 것이기 때문에 백만, 천만 개의 문서에 대해 학습된 일반적으로 다운받아서 사용할 수 있는 것과는 다르다는 것을 알아두어야 합니다.\n",
    "\n",
    "\n",
    ">### Rmsprop을 활용한 파라미터 업데이트\n",
    ">Rmsprop의 기본적인 아이디어는, 이전 gradient들의 합에 따라 파라미터별로 learning rate을 조정하는 것입니다. 직관적으로 이해해보면, 자주 등장하는 특징(feature)들은 작은 learning rate를 갖게 되고 (gradient들의 합이 작기 때문에), 드문드문 등장하는 특징들은 큰 learning rate를 갖게 됩니다.\n",
    "\n",
    "http://aikorea.org/blog/rnn-tutorial-4/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building The Website\n",
    "\n",
    "많은 헤드라인이 좋았지만 몇몇은 말이 되지 않았다. 이런 나쁜 결과를 걸러 내기 위해서 Reddit 과 비슷한 일을 했고 crowd source를 통해 이런 문제를 해결 했다.\n",
    "\n",
    "결국 나는 http://clickotron.com/ 라는 사이트를 만들었고 RNN으로 만들어진 첫번째 기사일 것이다 새로운 기사는 20분마다 한개씩 쓰여진다.\n",
    "\n",
    "<img src=\"https://larseidnes.files.wordpress.com/2015/09/screenshot.png?w=1008\" width=500 />\n",
    "\n",
    "\n",
    "어떤 유저도 vote up down 을 할 수 있다. 각각의 기사는 스코어를 가지는데 vote된 숫자와 조회수에 따라 스코어를 가진다. 이 스코어에 따라 front page를 ordering 한다. clickbaitiness 과 freshness 의 절충안을 위해서 해커스 뉴스의 알고리즘을 사용한다.\n",
    "\n",
    "<img src=\"https://larseidnes.files.wordpress.com/2015/09/points.png?w=1008\" width=300 />\n",
    "\n",
    "이 기사들은 세가지의 language models 로 나뉘어 지는데 하나는 헤드라인 하나는 기사 내용, 하나는 작성자 이름이다\n",
    "\n",
    "기사 내용은 헤드라인을 seed로 사용하는 NN이며 그 결과 헤드라인과의 내용적 일관성을 보인다. 기사 내용을 학습시키는 동안은 헤드라인을 사용하지 않았다.\n",
    "\n",
    "작성자 이름을 위해서 character level LSTM-RNN 를 이용했다. 미국의 모든 first name, last name의 corpus를 이용해 학습 시켰다. 모델은 이름의 목록을 생성하도록 했고 이 목록의 이름중 원래 corpus에 last, first name 둘다 없는 이름만 남도록 필터링 했다. 이것은 그럴듯하고 원래 없던 이름을 만들어 냈다. Flodrice Golpo and Richaldo Aariza 같은\n",
    "\n",
    "마지막으로 각각의 기사의 그림은 Wikimedia API에 headline text를 검색함으로써 가져왔다. \n",
    "\n",
    "종합적으로 우리의 모델은 쓸모없는 기사를 무한히 만들어 냈고 아무 비용도 안들었다. 경제학 수업때 들은것을 기억해보면 이건 쓸모없는 기사의 가치를 0으로 내릴것이고 사람들이 더 좋은 기사를 만들도록 할것이다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 부록 LSTM\n",
    "번역 http://www.whydsp.org/280\n",
    "원문 http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
