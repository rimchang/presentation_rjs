{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8. Sentiment Analyser Application for Movie Reviews\n",
    "In this chapter, we describe an application to determine the sentiment of movie reviews using algorithms and methods described throughout the book. In addition, the Scrapy library will be used to collect reviews from different websites through a search engine API (Bing search engine). The text and the title of the movie review is extracted using the newspaper library or following some pre-defined extraction rules of an HTML format page. The sentiment of each review is determined using a naive Bayes classifier on the most informative words (using the X2 measure) in the same way as in Chapter 4, Web Mining Techniques. Also, the rank of each page related to each movie query is calculated for completeness using the PageRank algorithm discussed in Chapter 4, Web Mining Techniques. This chapter will discuss the code used to build the application, including the Django models and views and the Scrapy scraper is used to collect data from the web pages of the movie reviews. We start by giving an example of what the web application will be and explaining the search engine API used and how we include it in the application. We then describe how we collect the movie reviews, integrating the Scrapy library into Django, the models to store the data, and the main commands to manage the application. All the code discussed in this chapter is available in the GitHub repository of the author inside the chapter_8 folder at https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_8.\n",
    "\n",
    "# Application usage overview\n",
    "The home web page is as follows:\n",
    "\n",
    "Application usage overview\n",
    "The user can type in the movie name, if they want to know the review's sentiments and relevance. For example, we look for Batman vs Superman Dawn of Justice in the following screenshot:\n",
    "\n",
    "Application usage overview\n",
    "The application collects and scrapes 18 reviews from the Bing search engine and, using the Scrapy library, it analyzes their sentiment (15 positive and 3 negative). All data is stored in Django models, ready to be used to calculate the relevance of each page using the PageRank algorithm (the links at the bottom of the page as seen in the preceding screenshot). In this case, using the PageRank algorithm, we have the following:\n",
    "\n",
    "Application usage overview\n",
    "This is a list of the most relevant pages to our movie review search, setting a depth parameter 2 on the scraping crawler (refer the following section for further details). Note that to have a good result on page relevance, you have to crawl thousands of pages (the preceding screenshot shows results for around 50 crawled pages).\n",
    "\n",
    "To write the application, we start the server as usual (see Chapter 6, Getting Started with Django, and Chapter 7, Movie Recommendation System Web Application) and the main app in Django. First, we create a folder to store all our codes, movie_reviews_analyzer_app, and then we initialize Django using the following command:\n",
    "```python\n",
    "mkdir  movie_reviews_analyzer_app\n",
    "cd  movie_reviews_analyzer_app\n",
    "django-admin startproject webmining_server\n",
    "python manage.py startapp startapp pages\n",
    "```\n",
    "We set the settings in the .py file as we did in the Settings section of Chapter 6, Getting Started with Django, and the Application Setup section of Chapter 7, Movie Recommendation System Web Application (of course, in this case the name is webmining_server instead of server_movierecsys).\n",
    "\n",
    "The sentiment analyzer application has the main views in the .py file in the main webmining_server folder instead of the app (pages) folder as we did previously (see Chapter 6, Getting Started with Django, and Chapter 7, Movie Recommendation System Web Application), because the functions now refer more to the general functioning of the server instead of the specific app (pages).\n",
    "\n",
    "The last operation to make the web service operational is to create a superuser account and go live with the server:\n",
    "```python\n",
    "python manage.py createsuperuser (admin/admin)\n",
    "python manage.py runserver\n",
    "```\n",
    "Now that the structure of the application has been explained, we can discuss the different parts in more detail starting from the search engine API used to collect URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search engine choice and the application code\n",
    "\n",
    "Since scraping directly from the most relevant search engines such as Google, Bing, Yahoo, and others is against their term of service, we need to take initial review pages from their REST API (using scraping services such as Crawlera, http://crawlera.com/, is also possible). We decided to use the Bing service, which allows 5,000 queries per month for free.\n",
    "\n",
    "In order to do that, we register to the Microsoft Service to obtain the key needed to allow the search. Briefly, we followed these steps:\n",
    "\n",
    "- Register online on https://datamarket.azure.com.\n",
    "- In My Account, take the Primary Account Key.\n",
    "- Register a new application (under DEVELOPERS | REGISTER; put Redirect URI: https://www.bing.com)\n",
    "After that, we can write a function that retrieves as many URLs relevant to our query as we want:\n",
    "```python\n",
    "num_reviews = 30 \n",
    "def bing_api(query):\n",
    "    keyBing = API_KEY        # get Bing key from: https://datamarket.azure.com/account/keys\n",
    "    credentialBing = 'Basic ' + (':%s' % keyBing).encode('base64')[:-1] # the \"-1\" is to remove the trailing \"\\n\" which encode adds\n",
    "    searchString = '%27X'+query.replace(\" \",'+')+'movie+review%27'\n",
    "    top = 50#maximum allowed by Bing\n",
    "    \n",
    "    reviews_urls = []\n",
    "    if num_reviews<top:\n",
    "        offset = 0\n",
    "        url = 'https://api.datamarket.azure.com/Bing/Search/Web?' + \\\n",
    "              'Query=%s&$top=%d&$skip=%d&$format=json' % (searchString, num_reviews, offset)\n",
    "\n",
    "        request = urllib2.Request(url)\n",
    "        request.add_header('Authorization', credentialBing)\n",
    "        requestOpener = urllib2.build_opener()\n",
    "        response = requestOpener.open(request)\n",
    "        results = json.load(response)\n",
    "        reviews_urls = [ d['Url'] for d in results['d']['results']]\n",
    "    else:\n",
    "        nqueries = int(float(num_reviews)/top)+1\n",
    "        for i in xrange(nqueries):\n",
    "            offset = top*i\n",
    "            if i==nqueries-1:\n",
    "                top = num_reviews-offset\n",
    "                url = 'https://api.datamarket.azure.com/Bing/Search/Web?' + \\\n",
    "                      'Query=%s&$top=%d&$skip=%d&$format=json' % (searchString, top, offset)\n",
    "\n",
    "                request = urllib2.Request(url)\n",
    "                request.add_header('Authorization', credentialBing)\n",
    "                requestOpener = urllib2.build_opener()\n",
    "                response = requestOpener.open(request) \n",
    "            else:\n",
    "                top=50\n",
    "                url = 'https://api.datamarket.azure.com/Bing/Search/Web?' + \\\n",
    "                      'Query=%s&$top=%d&$skip=%d&$format=json' % (searchString, top, offset)\n",
    "\n",
    "                request = urllib2.Request(url)\n",
    "                request.add_header('Authorization', credentialBing)\n",
    "                requestOpener = urllib2.build_opener()\n",
    "                response = requestOpener.open(request) \n",
    "            results = json.load(response)\n",
    "            reviews_urls += [ d['Url'] for d in results['d']['results']]\n",
    "    return reviews_urls\n",
    "    ```\n",
    "The API_KEY parameter is taken from the Microsoft account, query is a string which specifies the movie name, and num_reviews = 30 is the number of URLs returned in total from the Bing API. With the list of URLs that contain the reviews, we can now set up a scraper to extract from each web page the title and the review text using Scrapy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy setup and the application code\n",
    "Scrapy is a Python library is used to extract content from web pages or to crawl pages linked to a given web page (see the Web crawlers (or spiders) section of Chapter 4, Web Mining Techniques, for more details). To install the library, type the following in the terminal:\n",
    "```python\n",
    "sudo pip install Scrapy \n",
    "```\n",
    "Install the executable in the bin folder:\n",
    "```python\n",
    "sudo easy_install scrapy\n",
    "```\n",
    "From the movie_reviews_analyzer_app folder, we initialize our Scrapy project as follows:\n",
    "```python\n",
    "scrapy startproject scrapy_spider\n",
    "```\n",
    "This command will create the following tree inside the scrapy_spider folder:\n",
    "```python\n",
    "├── __init__.py\n",
    "├── items.py\n",
    "├── pipelines.py\n",
    "├── settings.py ",
    "├── spiders\n",
    "├── spiders\n",
    "│   ├── __init__.py\n",
    "```\n",
    "The pipelines.py and items.py files manage how the scraped data is stored and manipulated, and they will be discussed later in the Spiders and Integrate Django with Scrapy sections. The settings.py file sets the parameters each spider (or crawler) defined in the spiders folder uses to operate. In the following two sections, we describe the main parameters and spiders used in this application.\n",
    "\n",
    "# Scrapy settings\n",
    "The settings.py file collects all the parameters used by each spider in the Scrapy project to scrape web pages. The main parameters are as follows:\n",
    "\n",
    "- DEPTH_LIMIT: The number of subsequent pages crawled following an initial URL. The default is 0 and it means that no limit is set.\n",
    "- LOG_ENABLED: To allow/deny Scrapy to log on the terminal while executing default is true.\n",
    "- ITEM_PIPELINES = {'scrapy_spider.pipelines.ReviewPipeline': 1000,}: The path of the pipeline function to manipulate data extracted from each web page.\n",
    "- CONCURRENT_ITEMS = 200: The number of concurrent items processed in the pipeline.\n",
    "- CONCURRENT_REQUESTS = 5000: The maximum number of simultaneous requests handled by Scrapy.\n",
    "- CONCURRENT_REQUESTS_PER_DOMAIN = 3000: The maximum number of simultaneous requests handled by Scrapy for each specified domain.\n",
    "The larger the depth, more the pages are scraped and, consequently, the time needed to scrape increases. To speed up the process, you can set high value on the last three parameters. In this application (the spiders folder), we set two spiders: a scraper to extract data from each movie review URL (movie_link_results.py) and a crawler to generate a graph of webpages linked to the initial movie review URL (recursive_link_results.py).\n",
    "\n",
    "# Scraper\n",
    "The scraper on movie_link_results.py looks as follows:\n",
    "```python\n",
    "from newspaper import Article\n",
    "from urlparse import urlparse\n",
    "from scrapy.selector import Selector\n",
    "from scrapy import Spider\n",
    "from scrapy.spiders import BaseSpider,CrawlSpider, Rule\n",
    "from scrapy.http import Request\n",
    "from scrapy_spider import settings\n",
    "from scrapy_spider.items import PageItem,SearchItem\n",
    "\n",
    "unwanted_domains = ['youtube.com','www.youtube.com']\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def CheckQueryinReview(keywords,title,content):\n",
    "    content_list = map(lambda x:x.lower(),content.split(' '))\n",
    "    title_list = map(lambda x:x.lower(),title.split(' '))\n",
    "    words = content_list+title_list\n",
    "    for k in keywords:\n",
    "        if k in words:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "class Search(Spider):\n",
    "    name = 'scrapy_spider_reviews'\n",
    "    \n",
    "    def __init__(self,url_list,search_key):#specified by -a\n",
    "        self.search_key = search_key\n",
    "        self.keywords = [w.lower() for w in search_key.split(\" \") if w not in stopwords]\n",
    "        self.start_urls =url_list.split(',')\n",
    "        super(Search, self).__init__(url_list)\n",
    "    \n",
    "    def start_requests(self):\n",
    "        for url in self.start_urls:\n",
    "            yield Request(url=url, callback=self.parse_site,dont_filter=True)\n",
    "                        \n",
    "    def parse_site(self, response):\n",
    "        ## Get the selector for xpath parsing or from newspaper\n",
    "        \n",
    "        def crop_emptyel(arr):\n",
    "            return [u for u in arr if u!=' ']\n",
    "        \n",
    "        domain = urlparse(response.url).hostname\n",
    "        a = Article(response.url)\n",
    "        a.download()\n",
    "        a.parse()\n",
    "        title = a.title.encode('ascii','ignore').replace('\\n','')\n",
    "        sel = Selector(response)\n",
    "        if title==None:\n",
    "            title = sel.xpath('//title/text()').extract()\n",
    "            if len(title)>0:\n",
    "                title = title[0].encode('utf-8').strip().lower()\n",
    "                \n",
    "        content = a.text.encode('ascii','ignore').replace('\\n','')\n",
    "        if content == None:\n",
    "            content = 'none'\n",
    "            if len(crop_emptyel(sel.xpath('//div//article//p/text()').extract()))>1:\n",
    "                contents = crop_emptyel(sel.xpath('//div//article//p/text()').extract())\n",
    "                print 'divarticle'\n",
    "            ….\n",
    "            elif len(crop_emptyel(sel.xpath('/html/head/meta[@name=\"description\"]/@content').extract()))>0:\n",
    "                contents = crop_emptyel(sel.xpath('/html/head/meta[@name=\"description\"]/@content').extract())\n",
    "            content = ' '.join([c.encode('utf-8') for c in contents]).strip().lower()\n",
    "                \n",
    "        #get search item \n",
    "        search_item = SearchItem.django_model.objects.get(term=self.search_key)\n",
    "        #save item\n",
    "        if not PageItem.django_model.objects.filter(url=response.url).exists():\n",
    "            if len(content) > 0:\n",
    "                if CheckQueryinReview(self.keywords,title,content):\n",
    "                    if domain not in unwanted_domains:\n",
    "                        newpage = PageItem()\n",
    "                        newpage['searchterm'] = search_item\n",
    "                        newpage['title'] = title\n",
    "                        newpage['content'] = content\n",
    "                        newpage['url'] = response.url\n",
    "                        newpage['depth'] = 0\n",
    "                        newpage['review'] = True\n",
    "                        #newpage.save()\n",
    "                        return newpage  \n",
    "        else:\n",
    "            return null\n",
    "```\n",
    "We can see that the Spider class from scrapy is inherited by the Search class and the following standard methods have to be defined to override the standard methods:\n",
    "\n",
    "- __init__: The constructor of the spider needs to define the start_urls list that contains the URL to extract content from. In addition, we have custom variables such as search_key and keywords that store the information related to the query of the movie's title used on the search engine API.\n",
    "- start_requests: This function is triggered when spider is called and it declares what to do for each URL in - - the start_urls list; for each URL, the custom parse_site function will be called (instead of the default parse function).\n",
    "- parse_site: It is a custom function to parse data from each URL. To extract the title of the review and its text content, we used the newspaper library (sudo pip install newspaper) or, if it fails, we parse the HTML file directly using some defined rules to avoid the noise due to undesired tags (each rule structure is defined with the sel.xpath command). To achieve this result, we select some popular domains (rottentomatoes, cnn, and so on) and ensure the parsing is able to extract the content from these websites (not all the extraction rules are displayed in the preceding code but they can be found as usual in the GitHub file). The data is then stored in a page Django model using the related Scrapy item and the ReviewPipeline function (see the following section).\n",
    "- CheckQueryinReview: This is a custom function to check whether the movie title (from the query) is contained in the content or title of each web page.\n",
    "\n",
    "\n",
    "To run the spider, we need to type in the following command from the scrapy_spider (internal) folder:\n",
    "```python\n",
    "scrapy crawl scrapy_spider_reviews -a url_list=listname -a search_key=keyname\n",
    "```\n",
    "# Pipelines\n",
    "\n",
    "The pipelines define what to do when a new page is scraped by the spider. In the preceding case, the parse_site function returns a PageItem object, which triggers the following pipeline (pipelines.py):\n",
    "```python\n",
    "class ReviewPipeline(object):\n",
    "    def process_item(self, item, spider):\n",
    "        #if spider.name == 'scrapy_spider_reviews':#not working\n",
    "           item.save()\n",
    "           return item\n",
    "```\n",
    "This class simply saves each item (a new page in the spider notation).\n",
    "\n",
    "# Crawler\n",
    "As we showed in the overview (the preceding section), the relevance of the review is calculated using the PageRank algorithm after we have stored all the linked pages starting from the review's URL. The crawler recursive_link_results.py performs this operation:\n",
    "```python\n",
    "#from scrapy.spider import Spider\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.contrib.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.http import Request\n",
    "\n",
    "from scrapy_spider.items import PageItem,LinkItem,SearchItem\n",
    "\n",
    "class Search(CrawlSpider):\n",
    "    name = 'scrapy_spider_recursive'\n",
    "    \n",
    "    def __init__(self,url_list,search_id):#specified by -a\n",
    "    \n",
    "        #REMARK is allowed_domains is not set then ALL are allowed!!!\n",
    "        self.start_urls = url_list.split(',')\n",
    "        self.search_id = int(search_id)\n",
    "        \n",
    "        #allow any link but the ones with different font size(repetitions)\n",
    "        self.rules = (\n",
    "            Rule(LinkExtractor(allow=(),deny=('fontSize=*','infoid=*','SortBy=*', ),unique=True), callback='parse_item', follow=True), \n",
    "            )\n",
    "        super(Search, self).__init__(url_list)\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        sel = Selector(response)\n",
    "        \n",
    "        ## Get meta info from website\n",
    "        title = sel.xpath('//title/text()').extract()\n",
    "        if len(title)>0:\n",
    "            title = title[0].encode('utf-8')\n",
    "            \n",
    "        contents = sel.xpath('/html/head/meta[@name=\"description\"]/@content').extract()\n",
    "        content = ' '.join([c.encode('utf-8') for c in contents]).strip()\n",
    "\n",
    "        fromurl = response.request.headers['Referer']\n",
    "        tourl = response.url\n",
    "        depth = response.request.meta['depth']\n",
    "        \n",
    "        #get search item \n",
    "        search_item = SearchItem.django_model.objects.get(id=self.search_id)\n",
    "        #newpage\n",
    "        if not PageItem.django_model.objects.filter(url=tourl).exists():\n",
    "            newpage = PageItem()\n",
    "            newpage['searchterm'] = search_item\n",
    "            newpage['title'] = title\n",
    "            newpage['content'] = content\n",
    "            newpage['url'] = tourl\n",
    "            newpage['depth'] = depth\n",
    "            newpage.save()#cant use pipeline cause the execution can finish here\n",
    "        \n",
    "        #get from_id,to_id\n",
    "        from_page = PageItem.django_model.objects.get(url=fromurl)\n",
    "        from_id = from_page.id\n",
    "        to_page = PageItem.django_model.objects.get(url=tourl)\n",
    "        to_id = to_page.id\n",
    "        \n",
    "        #newlink\n",
    "        if not LinkItem.django_model.objects.filter(from_id=from_id).filter(to_id=to_id).exists():\n",
    "            newlink = LinkItem()\n",
    "            newlink['searchterm'] = search_item\n",
    "            newlink['from_id'] = from_id\n",
    "            newlink['to_id'] = to_id\n",
    "            newlink.save()\n",
    "```\n",
    "The CrawlSpider class from scrapy is inherited by the Search class, and the following standard methods have to be defined to override the standard methods (as for the spider case):\n",
    "\n",
    "- __init__: The is a constructor of the class. The start_urls parameter defines the starting URL from which the spider will start to crawl until the DEPTH_LIMIT value is reached. The rules parameter sets the type of URL allowed/denied to scrape (in this case, the same page but with different font sizes is disregarded) and it defines the function to call to manipulate each retrieved page (parse_item). Also, a custom variable search_id is defined, which is needed to store the ID of the query within the other data.\n",
    "- parse_item: This is a custom function called to store the important data from each retrieved page. A new Django item of the Page model (see the following section) from each page is created, which contains the title and content of the page (using the xpath HTML parser). To perform the PageRank algorithm, the connection from the page that links to each page and the page itself is saved as an object of the Link model using the related Scrapy item (see the following sections).\n",
    "\n",
    "\n",
    "To run the crawler, we need to type the following from the (internal) scrapy_spider folder:\n",
    "```python\n",
    "scrapy crawl scrapy_spider_recursive -a url_list=listname -a search_id=keyname\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Django models\n",
    "The data collected using the spiders needs to be stored in a database. In Django, the database tables are called models and defined in the models.py file (within the pages folder). The content of this file is as follows:\n",
    "```python\n",
    "from django.db import models\n",
    "from django.conf import settings\n",
    "from django.utils.translation import ugettext_lazy as _\n",
    "\n",
    "class SearchTerm(models.Model):\n",
    "    term = models.CharField(_('search'), max_length=255)\n",
    "    num_reviews = models.IntegerField(null=True,default=0)\n",
    "    #display term on admin panel\n",
    "    def __unicode__(self):\n",
    "            return self.term\n",
    "\n",
    "class Page(models.Model):\n",
    "     searchterm = models.ForeignKey(SearchTerm, related_name='pages',null=True,blank=True)\n",
    "     url = models.URLField(_('url'), default='', blank=True)\n",
    "     title = models.CharField(_('name'), max_length=255)\n",
    "     depth = models.IntegerField(null=True,default=-1)\n",
    "     html = models.TextField(_('html'),blank=True, default='')\n",
    "     review = models.BooleanField(default=False)\n",
    "     old_rank = models.FloatField(null=True,default=0)\n",
    "     new_rank = models.FloatField(null=True,default=1)\n",
    "     content = models.TextField(_('content'),blank=True, default='')\n",
    "     sentiment = models.IntegerField(null=True,default=100)\n",
    "     \n",
    "class Link(models.Model):\n",
    "     searchterm = models.ForeignKey(SearchTerm, related_name='links',null=True,blank=True)\n",
    "     from_id = models.IntegerField(null=True)\n",
    "     to_id = models.IntegerField(null=True)\n",
    "```\n",
    "Each movie title typed on the home page of the application is stored in the SearchTerm model, while the data of each web page is collected in an object of the Page model. Apart from the content field (HTML, title, URL, content), the sentiment of the review and the depth in graph network are recorded (a Boolean also indicates if the web page is a movie review page or simply a linked page). The Link model stores all the graph links between pages, which are then used by the PageRank algorithm to calculate the relevance of the reviews web pages. Note that the Page model and the Link model are both linked to the related SearchTerm through a foreign key. As usual, to write these models as database tables, we type the following commands:\n",
    "```python\n",
    "python manage.py makemigrations\n",
    "python manage.py migrate\n",
    "```\n",
    "To populate these Django models, we need to make Scrapy interact with Django, and this is the subject of the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# integrating Django with Scrapy\n",
    "\n",
    "To make paths easy to call, we remove the external scrapy_spider folder so that inside the movie_reviews_analyzer_app, the webmining_server folder is at the same level as the scrapy_spider folder:\n",
    "```python\n",
    "├── db.sqlite3\n",
    "├── scrapy.cfg\n",
    "├── scrapy_spider\n",
    "│   ├── ...\n",
    "│   ├── spiders\n",
    "│   │   ...\n",
    "└── webmining_server\n",
    "```\n",
    "We set the Django path into the Scrapy settings.py file:\n",
    "```python\n",
    "# Setting up django's project full path.\n",
    "import sys\n",
    "sys.path.insert(0, BASE_DIR+'/webmining_server')\n",
    "# Setting up django's settings module name.\n",
    "os.environ['DJANGO_SETTINGS_MODULE'] = 'webmining_server.settings'\n",
    "#import django to load models(otherwise AppRegistryNotReady: Models aren't loaded yet):\n",
    "import django\n",
    "django.setup()\n",
    "```\n",
    "Now we can install the library that will allow managing Django models from Scrapy:\n",
    "```python\n",
    "sudo pip install scrapy-djangoitem\n",
    "```\n",
    "In the items.py file, we write the links between Django models and Scrapy items as follows:\n",
    "```python\n",
    "from scrapy_djangoitem import DjangoItem\n",
    "from pages.models import Page,Link,SearchTerm\n",
    "\n",
    "class SearchItem(DjangoItem):\n",
    "    django_model = SearchTerm\n",
    "class PageItem(DjangoItem):\n",
    "    django_model = Page\n",
    "class LinkItem(DjangoItem):\n",
    "    django_model = Link\n",
    "```\n",
    "Each class inherits the DjangoItem class so that the original Django models declared with the django_model variable are automatically linked. The Scrapy project is now completed so we can continue our discussion explaining the Django codes that handle the data extracted by Scrapy and the Django commands needed to manage the applications.\n",
    "\n",
    "# Commands (sentiment analysis model and delete queries)\n",
    "The application needs to manage some operations that are not allowed to the final user of the service, such as defining a sentiment analysis model and deleting a query of a movie in order to redo it instead of retrieving the existing data from memory. The following sections will explain the commands to perform these actions.\n",
    "\n",
    "# Sentiment analysis model loader\n",
    "The final goal of this application is to determine the sentiment (positive or negative) of the movie reviews. To achieve that, a sentiment classifier must be built using some external data, and then it should be stored in memory (cache) to be used by each query request. This is the purpose of the load_sentimentclassifier.py command displayed hereafter:\n",
    "\n",
    "```python\n",
    "import nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "import collections\n",
    "from django.core.management.base import BaseCommand, CommandError\n",
    "from optparse import make_option\n",
    "from django.core.cache import cache\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "method_selfeatures = 'best_words_features'\n",
    "\n",
    "class Command(BaseCommand):\n",
    "    option_list = BaseCommand.option_list + (\n",
    "                make_option('-n', '--num_bestwords',\t\n",
    "                             dest='num_bestwords', type='int',\n",
    "                             action='store',\n",
    "                             help=('number of words with high information')),)\n",
    "    \n",
    "    def handle(self, *args, **options):\n",
    "         num_bestwords = options['num_bestwords']\n",
    "         self.bestwords = self.GetHighInformationWordsChi(num_bestwords)\n",
    "         clf = self.train_clf(method_selfeatures)\n",
    "         cache.set('clf',clf)\n",
    "         cache.set('bestwords',self.bestwords)\n",
    "```\n",
    "At the beginning of the file, the variable method_selfeatures sets the method of feature selection (in this case, the features are the words in the reviews; see Chapter 4, Web Mining Techniques, for further details) used to train the classifier train_clf. The maximum number of best words (features) is defined by the input parameter num_bestwords. The classifier and the best features (bestwords) are then stored in the cache ready to be used by the application (using the cache module). The classifier and the methods to select the best words (features) are as follows:\n",
    "```python\n",
    "    def train_clf(method):\n",
    "        negidxs = movie_reviews.fileids('neg')\n",
    "        posidxs = movie_reviews.fileids('pos')\n",
    "        if method=='stopword_filtered_words_features':\n",
    "            negfeatures = [(stopword_filtered_words_features(movie_reviews.words(fileids=[file])), 'neg') for file in negidxs]\n",
    "            posfeatures = [(stopword_filtered_words_features(movie_reviews.words(fileids=[file])), 'pos') for file in posidxs]\n",
    "        elif method=='best_words_features':\n",
    "            negfeatures = [(best_words_features(movie_reviews.words(fileids=[file])), 'neg') for file in negidxs]\n",
    "            posfeatures = [(best_words_features(movie_reviews.words(fileids=[file])), 'pos') for file in posidxs]\n",
    "        elif method=='best_bigrams_words_features':\n",
    "            negfeatures = [(best_bigrams_words_features(movie_reviews.words(fileids=[file])), 'neg') for file in negidxs]\n",
    "            posfeatures = [(best_bigrams_words_features(movie_reviews.words(fileids=[file])), 'pos') for file in posidxs]\n",
    "            \n",
    "        trainfeatures = negfeatures + posfeatures\n",
    "        clf = NaiveBayesClassifier.train(trainfeatures)\n",
    "        return clf\n",
    "\n",
    "    def stopword_filtered_words_features(self,words):\n",
    "        return dict([(word, True) for word in words if word not in stopwords])\n",
    "\n",
    "    #eliminate Low Information Features\n",
    "    def GetHighInformationWordsChi(self,num_bestwords):\n",
    "        word_fd = FreqDist()\n",
    "        label_word_fd = ConditionalFreqDist()\n",
    "\n",
    "        for word in movie_reviews.words(categories=['pos']):\n",
    "            word_fd[word.lower()] +=1\n",
    "            label_word_fd['pos'][word.lower()] +=1\n",
    "\n",
    "        for word in movie_reviews.words(categories=['neg']):\n",
    "            word_fd[word.lower()] +=1\n",
    "            label_word_fd['neg'][word.lower()] +=1\n",
    "\n",
    "        pos_word_count = label_word_fd['pos'].N()\n",
    "        neg_word_count = label_word_fd['neg'].N()\n",
    "        total_word_count = pos_word_count + neg_word_count\n",
    "\n",
    "        word_scores = {}\n",
    "        for word, freq in word_fd.iteritems():\n",
    "            pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],\n",
    "                (freq, pos_word_count), total_word_count)\n",
    "            neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],\n",
    "                (freq, neg_word_count), total_word_count)\n",
    "            word_scores[word] = pos_score + neg_score\n",
    "\n",
    "        best = sorted(word_scores.iteritems(), key=lambda (w,s): s, reverse=True)[:num_bestwords]\n",
    "        bestwords = set([w for w, s in best])\n",
    "        return bestwords\n",
    "\n",
    "    def best_words_features(self,words):\n",
    "        return dict([(word, True) for word in words if word in self.bestwords])\n",
    "    \n",
    "    def best_bigrams_word_features(self,words, measure=BigramAssocMeasures.chi_sq, nbigrams=200):\n",
    "        bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "        bigrams = bigram_finder.nbest(measure, nbigrams)\n",
    "        d = dict([(bigram, True) for bigram in bigrams])\n",
    "        d.update(best_words_features(words))\n",
    "        return d\n",
    "```\n",
    "Three methods are written to select words in the preceding code:\n",
    "\n",
    "- stopword_filtered_words_features: Eliminates the stopwords using the Natural Language Toolkit (NLTK) list of conjunctions and considers the rest as relevant words\n",
    "- best_words_features: Using the X2 measure (NLTK library), the most informative words related to positive or negative reviews are selected (see Chapter 4, Web Mining Techniques, for further details)\n",
    "- best_bigrams_word_features: Uses the X2 measure (NLTK library) to find the 200 most informative bigrams from the set of words (see Chapter 4, Web Mining Techniques, for further details)\n",
    "\n",
    "\n",
    "The chosen classifier is the Naive Bayes algorithm (see Chapter 3, Supervised Machine Learning) and the labeled text (positive, negative sentiment) is taken from the NLTK.corpus of movie_reviews. To install it, open a terminal in Python and install movie_reviews from corpus:\n",
    "```python\n",
    "nltk.download()--> corpora/movie_reviews corpus\n",
    "```\n",
    "# Deleting an already performed query\n",
    "Since we can specify different parameters (such as the feature selection method, the number of best words, and so on), we may want to perform and store again the sentiment of the reviews with different values. The delete_query command is needed for this purpose and it is as follows:\n",
    "```python\n",
    "from pages.models import Link,Page,SearchTerm\n",
    "from django.core.management.base import BaseCommand, CommandError\n",
    "from optparse import make_option\n",
    "\n",
    "class Command(BaseCommand):\n",
    "    option_list = BaseCommand.option_list + (\n",
    "                make_option('-s', '--searchid',\n",
    "                             dest='searchid', type='int',\n",
    "                             action='store',\n",
    "                             help=('id of the search term to delete')),)\n",
    "\n",
    "    def handle(self, *args, **options):\n",
    "         searchid = options['searchid']\n",
    "         if searchid == None:\n",
    "             print \"please specify searchid: python manage.py --searchid=--\"\n",
    "             #list\n",
    "             for sobj in SearchTerm.objects.all():\n",
    "                 print 'id:',sobj.id,\"  term:\",sobj.term\n",
    "         else:\n",
    "             print 'delete...'\n",
    "             search_obj = SearchTerm.objects.get(id=searchid)\n",
    "             pages = search_obj.pages.all()\n",
    "             pages.delete()\n",
    "             links = search_obj.links.all()\n",
    "             links.delete()\n",
    "             search_obj.delete()\n",
    "```\n",
    "If we run the command without specifying the searchid (the ID of the query), the list of all the queries and related IDs will be shown. After that we can choose which query we want to delete by typing the following:\n",
    "```python\n",
    "python manage.py delete_query --searchid=VALUE\n",
    "```\n",
    "\n",
    "We can use the cached sentiment analysis model to show the user the online sentiment of the chosen movie, as we explain in the following section.\n",
    "\n",
    "# Sentiment reviews analyser – Django views and HTML\n",
    "\n",
    "Most of the code explained in this chapter (commands, Bing search engine, Scrapy, and Django models) is used in the function analyzer in views.py to power the home webpage shown in the Application usage overview section (after declaring the URL in the urls.py file as\n",
    "```python\n",
    "url(r'^','webmining_server.views.analyzer')).\n",
    "```\n",
    "\n",
    "```python\n",
    "def analyzer(request):\n",
    "    context = {}\n",
    "\n",
    "    if request.method == 'POST':\n",
    "        post_data = request.POST\n",
    "        query = post_data.get('query', None)\n",
    "        if query:\n",
    "            return redirect('%s?%s' % (reverse('webmining_server.views.analyzer'),\n",
    "                                urllib.urlencode({'q': query})))   \n",
    "    elif request.method == 'GET':\n",
    "        get_data = request.GET\n",
    "        query = get_data.get('q')\n",
    "        if not query:\n",
    "            return render_to_response(\n",
    "                'movie_reviews/home.html', RequestContext(request, context))\n",
    "\n",
    "        context['query'] = query\n",
    "        stripped_query = query.strip().lower()\n",
    "        urls = []\n",
    "        \n",
    "        if test_mode:\n",
    "           urls = parse_bing_results()\n",
    "        else:\n",
    "           urls = bing_api(stripped_query)\n",
    "           \n",
    "        if len(urls)== 0:\n",
    "           return render_to_response(\n",
    "               'movie_reviews/noreviewsfound.html', RequestContext(request, context))\n",
    "        if not SearchTerm.objects.filter(term=stripped_query).exists():\n",
    "           s = SearchTerm(term=stripped_query)\n",
    "           s.save()\n",
    "           try:\n",
    "               #scrape\n",
    "               cmd = 'cd ../scrapy_spider & scrapy crawl scrapy_spider_reviews -a url_list=%s -a search_key=%s' %('\\\"'+str(','.join(urls[:num_reviews]).encode('utf-8'))+'\\\"','\\\"'+str(stripped_query)+'\\\"')\n",
    "               os.system(cmd)\n",
    "           except:\n",
    "               print 'error!'\n",
    "               s.delete()\n",
    "        else:\n",
    "           #collect the pages already scraped \n",
    "           s = SearchTerm.objects.get(term=stripped_query)\n",
    "           \n",
    "        #calc num pages\n",
    "        pages = s.pages.all().filter(review=True)\n",
    "        if len(pages) == 0:\n",
    "           s.delete()\n",
    "           return render_to_response(\n",
    "               'movie_reviews/noreviewsfound.html', RequestContext(request, context))\n",
    "               \n",
    "        s.num_reviews = len(pages)\n",
    "        s.save()\n",
    "         \n",
    "        context['searchterm_id'] = int(s.id)\n",
    "\n",
    "        #train classifier with nltk\n",
    "        def train_clf(method):\n",
    "            ...           \n",
    "        def stopword_filtered_words_features(words):\n",
    "            ... \n",
    "        #Eliminate Low Information Features\n",
    "        def GetHighInformationWordsChi(num_bestwords):\n",
    "            ...            \n",
    "        bestwords = cache.get('bestwords')\n",
    "        if bestwords == None:\n",
    "            bestwords = GetHighInformationWordsChi(num_bestwords)\n",
    "        def best_words_features(words):\n",
    "            ...       \n",
    "        def best_bigrams_words_features(words, measure=BigramAssocMeasures.chi_sq, nbigrams=200):\n",
    "            ...\n",
    "        clf = cache.get('clf')\n",
    "        if clf == None:\n",
    "            clf = train_clf(method_selfeatures)\n",
    "\n",
    "        cntpos = 0\n",
    "        cntneg = 0\n",
    "        for p in pages:\n",
    "            words = p.content.split(\" \")\n",
    "            feats = best_words_features(words)#bigram_word_features(words)#stopword_filtered_word_feats(words)\n",
    "            #print feats\n",
    "            str_sent = clf.classify(feats)\n",
    "            if str_sent == 'pos':\n",
    "               p.sentiment = 1\n",
    "               cntpos +=1\n",
    "            else:\n",
    "               p.sentiment = -1\n",
    "               cntneg +=1\n",
    "            p.save()\n",
    "\n",
    "        context['reviews_classified'] = len(pages)\n",
    "        context['positive_count'] = cntpos\n",
    "        context['negative_count'] = cntneg\n",
    "        context['classified_information'] = True\n",
    "    return render_to_response(\n",
    "        'movie_reviews/home.html', RequestContext(request, context))\n",
    "```\n",
    "The inserted movie title is stored in the query variable and sent to the bing_api function to collect review's URL. The URL are then scraped calling Scrapy to find the review texts, which are processed using the clf classifier model and the selected most informative words (bestwords) retrieved from the cache (or the same model is generated again in case the cache is empty). The counts of the predicted sentiments of the reviews (positive_counts, negative_counts, and reviews_classified) are then sent back to the home.html (the templates folder) page, which uses the following Google pie chart code:\n",
    "```python\n",
    "        <h2 align = Center>Movie Reviews Sentiment Analysis</h2>\n",
    "        <div class=\"row\">\n",
    "        <p align = Center><strong>Reviews Classified : {{ reviews_classified }}</strong></p>\n",
    "        <p align = Center><strong>Positive Reviews : {{ positive_count }}</strong></p>\n",
    "        <p align = Center><strong> Negative Reviews : {{ negative_count }}</strong></p>\n",
    "        </div> \n",
    "  <section>\n",
    "      <script type=\"text/javascript\" src=\"https://www.google.com/jsapi\"></script>\n",
    "      <script type=\"text/javascript\">\n",
    "        google.load(\"visualization\", \"1\", {packages:[\"corechart\"]});\n",
    "        google.setOnLoadCallback(drawChart);\n",
    "        function drawChart() {\n",
    "          var data = google.visualization.arrayToDataTable([\n",
    "            ['Sentiment', 'Number'],\n",
    "            ['Positive',     {{ positive_count }}],\n",
    "            ['Negative',      {{ negative_count }}]\n",
    "          ]);\n",
    "          var options = { title: 'Sentiment Pie Chart'};\n",
    "          var chart = new google.visualization.PieChart(document.getElementById('piechart'));\n",
    "          chart.draw(data, options);\n",
    "        }\n",
    "      </script>\n",
    "        <p align =\"Center\" id=\"piechart\" style=\"width: 900px; height: 500px;display: block; margin: 0 auto;text-align: center;\" ></p>\n",
    "      </div>\n",
    "```\n",
    "The function drawChart calls the Google PieChart visualization function, which takes as input the data (the positive and negative counts) to create the pie chart. To have more details about how the HTML code interacts with the Django views, refer to Chapter 6, Getting Started with Django, in the URL and views behind html web pages section. From the result page with the sentiment counts (see the Application usage overview section), the PagerRank relevance of the scraped reviews can be calculated using one of the two links at the bottom of the page. The Django code behind this operation is discussed in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PageRank: Django view and the algorithm code\n",
    "To rank the importance of the online reviews, we have implemented the PageRank algorithm (see Chapter 4, Web Mining Techniques, in the Ranking: PageRank algorithm section) into the application. The pgrank.py file in the pgrank folder within the webmining_server folder implements the algorithm that follows:\n",
    "```python\n",
    "from pages.models import Page,SearchTerm\n",
    "\n",
    "num_iterations = 100000\n",
    "eps=0.0001\n",
    "D = 0.85\n",
    "\n",
    "def pgrank(searchid):\n",
    "    s = SearchTerm.objects.get(id=int(searchid))\n",
    "    links = s.links.all()\n",
    "    from_idxs = [i.from_id for i in links ]\n",
    "    # Find the idxs that receive page rank \n",
    "    links_received = []\n",
    "    to_idxs = []\n",
    "    for l in links:\n",
    "        from_id = l.from_id\n",
    "        to_id = l.to_id\n",
    "        if from_id not in from_idxs: continue\n",
    "        if to_id  not in from_idxs: continue\n",
    "        links_received.append([from_id,to_id])\n",
    "        if to_id  not in to_idxs: to_idxs.append(to_id)\n",
    "        \n",
    "    pages = s.pages.all()\n",
    "    prev_ranks = dict()\n",
    "    for node in from_idxs:\n",
    "        ptmp  = Page.objects.get(id=node)\n",
    "        prev_ranks[node] = ptmp.old_rank\n",
    "        \n",
    "    conv=1.\n",
    "    cnt=0\n",
    "    while conv>eps or cnt<num_iterations:\n",
    "        next_ranks = dict()\n",
    "        total = 0.0\n",
    "        for (node,old_rank) in prev_ranks.items():\n",
    "            total += old_rank\n",
    "            next_ranks[node] = 0.0\n",
    "        \n",
    "        #find the outbound links and send the pagerank down to each of them\n",
    "        for (node, old_rank) in prev_ranks.items():\n",
    "            give_idxs = []\n",
    "            for (from_id, to_id) in links_received:\n",
    "                if from_id != node: continue\n",
    "                if to_id  not in to_idxs: continue\n",
    "                give_idxs.append(to_id)\n",
    "            if (len(give_idxs) < 1): continue\n",
    "            amount = D*old_rank/len(give_idxs)\n",
    "            for id in give_idxs:\n",
    "                next_ranks[id] += amount\n",
    "        tot = 0\n",
    "        for (node,next_rank) in next_ranks.items():\n",
    "            tot += next_rank\n",
    "        const = (1-D)/ len(next_ranks)\n",
    "        \n",
    "        for node in next_ranks:\n",
    "            next_ranks[node] += const\n",
    "        \n",
    "        tot = 0\n",
    "        for (node,old_rank) in next_ranks.items():\n",
    "            tot += next_rank\n",
    "        \n",
    "        difftot = 0\n",
    "        for (node, old_rank) in prev_ranks.items():\n",
    "            new_rank = next_ranks[node]\n",
    "            diff = abs(old_rank-new_rank)\n",
    "            difftot += diff\n",
    "        conv= difftot/len(prev_ranks)\n",
    "        cnt+=1\n",
    "        prev_ranks = next_ranks\n",
    "\n",
    "    for (id,new_rank) in next_ranks.items():\n",
    "        ptmp = Page.objects.get(id=id)\n",
    "        url = ptmp.url\n",
    "    \n",
    "    for (id,new_rank) in next_ranks.items():\n",
    "        ptmp = Page.objects.get(id=id)\n",
    "        ptmp.old_rank = ptmp.new_rank\n",
    "        ptmp.new_rank = new_rank\n",
    "        ptmp.save()\n",
    "```\n",
    "\n",
    "This code takes all the links stores associated with the given SearchTerm object and implements the PageRank score for each page i at time t, where P(i) is given by the recursive equation:\n",
    "\n",
    "<img src=\"./picture/B05143_08_05.jpg\" width=300 />\n",
    "\n",
    "PageRank: Django view and the algorithm code\n",
    "Here, N is the total number of pages, and<img src=\"./picture/B05143_08_06.jpg\" width=100 /> PageRank: Django view and the algorithm code(Nj is the number of out links of page j) if page j points to i; otherwise, N is 0. The parameter D is the so-called damping factor (set to 0.85 in the preceding code), and it represents the probability to follow the transition given by the transition matrix A. The equation is iterated until the convergence parameter eps is satisfied or the maximum number of iterations, num_iterations, is reached. The algorithm is called by clicking either scrape and calculate page rank (may take a long time) or calculate page rank links at the bottom of the home.html page after the sentiment of the movie reviews has been displayed. The link is linked to the function pgrank_view in the views.py (through the declared URL in urls.py: url(r'^pg-rank/(?P<pk>\\d+)/','webmining_server.views.pgrank_view', name='pgrank_view')):\n",
    "```python\n",
    "def pgrank_view(request,pk): \n",
    "    context = {}\n",
    "    get_data = request.GET\n",
    "    scrape = get_data.get('scrape','False')\n",
    "    s = SearchTerm.objects.get(id=pk)\n",
    "    \n",
    "    if scrape == 'True':\n",
    "        pages = s.pages.all().filter(review=True)\n",
    "        urls = []\n",
    "        for u in pages:\n",
    "            urls.append(u.url)\n",
    "        #crawl\n",
    "        cmd = 'cd ../scrapy_spider & scrapy crawl scrapy_spider_recursive -a url_list=%s -a search_id=%s' %('\\\"'+str(','.join(urls[:]).encode('utf-8'))+'\\\"','\\\"'+str(pk)+'\\\"')\n",
    "        os.system(cmd)\n",
    "\n",
    "    links = s.links.all()\n",
    "    if len(links)==0:\n",
    "       context['no_links'] = True\n",
    "       return render_to_response(\n",
    "           'movie_reviews/pg-rank.html', RequestContext(request, context))\n",
    "    #calc pgranks\n",
    "    pgrank(pk)\n",
    "    #load pgranks in descending order of pagerank\n",
    "    pages_ordered = s.pages.all().filter(review=True).order_by('-new_rank')\n",
    "    context['pages'] = pages_ordered\n",
    "    \n",
    "    return render_to_response(\n",
    "        'movie_reviews/pg-rank.html', RequestContext(request, context)) \n",
    "```\n",
    "This code calls the crawler to collect all the linked pages to the reviews and calculate the PageRank scores using the code discussed earlier. Then the scores are displayed in the pg-rank.html page (in descending order by page rank score) as we showed in the Application usage overview section of this chapter. Since this function can take a long time to process (to crawl thousands of pages), the command run_scrapelinks.py has been written to run the Scrapy crawler (the reader is invited to read or modify the script as they like as an exercise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Admin and API\n",
    "As the last part of the chapter, we describe briefly some possible admin management of the model and the implementation of an API endpoint to retrieve the data processed by the application. In the pages folder, we can set two admin interfaces in the admin.py file to check the data collected by the SearchTerm and Page models:\n",
    "```python\n",
    "from django.contrib import admin\n",
    "from django_markdown.admin import MarkdownField, AdminMarkdownWidget\n",
    "from pages.models import SearchTerm,Page,Link\n",
    "\n",
    "class SearchTermAdmin(admin.ModelAdmin):\n",
    "    formfield_overrides = {MarkdownField: {'widget': AdminMarkdownWidget}}\n",
    "    list_display = ['id', 'term', 'num_reviews']\n",
    "    ordering = ['-id']\n",
    "    \n",
    "class PageAdmin(admin.ModelAdmin):\n",
    "    formfield_overrides = {MarkdownField: {'widget': AdminMarkdownWidget}}\n",
    "    list_display = ['id', 'searchterm', 'url','title','content']\n",
    "    ordering = ['-id','-new_rank']\n",
    "    \n",
    "admin.site.register(SearchTerm,SearchTermAdmin)\n",
    "admin.site.register(Page,PageAdmin)\n",
    "admin.site.register(Link)\n",
    "```\n",
    "Note that both SearchTermAdmin and PageAdmin display objects with decreasing ID (and new_rank in the case of PageAdmin). The following screenshot is an example:\n",
    "\n",
    "Admin and API\n",
    "Note that although it is not necessary, the Link model has also been included in the admin interface (admin.site.register(Link)). More interestingly, we can set up an API endpoint to retrieve the sentiment counts related to a movie's title. In the api.py file inside the pages folder, we can have the following:\n",
    "```python\n",
    "from rest_framework import views,generics\n",
    "from rest_framework.permissions import AllowAny\n",
    "from rest_framework.response import Response\n",
    "from rest_framework.pagination import PageNumberPagination\n",
    "from pages.serializers import SearchTermSerializer\n",
    "from pages.models import SearchTerm,Page\n",
    "\n",
    "class LargeResultsSetPagination(PageNumberPagination):\n",
    "    page_size = 1000\n",
    "    page_size_query_param = 'page_size'\n",
    "    max_page_size = 10000\n",
    "  \n",
    "class SearchTermsList(generics.ListAPIView):\n",
    "\n",
    "    serializer_class = SearchTermSerializer\n",
    "    permission_classes = (AllowAny,)\n",
    "    pagination_class = LargeResultsSetPagination\n",
    "    \n",
    "    def get_queryset(self):\n",
    "        return SearchTerm.objects.all()  \n",
    "        \n",
    "class PageCounts(views.APIView):\n",
    "\n",
    "    permission_classes = (AllowAny,)\n",
    "    def get(self,*args, **kwargs):\n",
    "        searchid=self.kwargs['pk']\n",
    "        reviewpages = Page.objects.filter(searchterm=searchid).filter(review=True)\n",
    "        npos = len([p for p in reviewpages if p.sentiment==1])\n",
    "        nneg = len(reviewpages)-npos\n",
    "        return Response({'npos':npos,'nneg':nneg})\n",
    "```\n",
    "The PageCounts class takes as input the ID of the search (the movie's title) and it returns the sentiments, that is, positive and negative counts, for the movie's reviews. To get the ID of earchTerm from a movie's title, you can either look at the admin interface or use the other API endpoint SearchTermsList; this simply returns the list of the movies' titles together with the associated ID. The serializer is set on the serializers.py file:\n",
    "```python\n",
    "from pages.models import SearchTerm\n",
    "from rest_framework import serializers\n",
    "        \n",
    "class SearchTermSerializer(serializers.HyperlinkedModelSerializer):\n",
    "    class Meta:\n",
    "        model = SearchTerm\n",
    "        fields = ('id', 'term')\n",
    "```\n",
    "To call these endpoints, we can again use the swagger interface (see Chapter 6, Getting Started with Django) or use the curl command in the terminal to make these calls. For instance:\n",
    "```python\n",
    "curl -X GET localhost:8000/search-list/\n",
    "{\"count\":7,\"next\":null,\"previous\":null,\"results\":[{\"id\":24,\"term\":\"the martian\"},{\"id\":27,\"term\":\"steve jobs\"},{\"id\":29,\"term\":\"suffragette\"},{\"id\":39,\"term\":\"southpaw\"},{\"id\":40,\"term\":\"vacation\"},{\"id\":67,\"term\":\"the revenant\"},{\"id\":68,\"term\":\"batman vs superman dawn of justice\"}]}\n",
    "```\n",
    "and\n",
    "```python\n",
    "curl -X GET localhost:8000/pages-sentiment/68/\n",
    "{\"nneg\":3,\"npos\":15}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this chapter, we described a movie review sentiment analyzer web application to make you familiar with some of the algorithms and libraries we discussed in Chapter 3, Supervised Machine Learning, Chapter 4, Web Mining Techniques, and Chapter 6, Getting Started with Django.\n",
    "\n",
    "This is the end of a journey: by reading this book and experimenting with the codes provided, you should have acquired significant practical knowledge about the most important machine learning algorithms used in the commercial environment nowadays.\n",
    "\n",
    "You should be now ready to develop your own web applications and ideas using Python and some machine learning algorithms, learned by reading this book. Many challenging data-related problems are present in the real world today, waiting to be solved by people who can grasp and apply the material treated in this book, and you, who have arrived at this point, are certainly one of those people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
