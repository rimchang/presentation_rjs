{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1, Introduction\n",
    "\n",
    "### given\n",
    "\n",
    "- 1) measurements of an agent's behavior over time, in a varety of circumstance\n",
    "- 2) if needed measurements of the sensory input to that agent\n",
    "- 3) if available a model of envirionment\n",
    "\n",
    "### goal\n",
    "\n",
    "find reward function\n",
    "\n",
    "- policy를 배우는 것이 아닌 optimal reward function을 학습하고 이를 통해 behavior 를 생성합니다.\n",
    "reward가 주어졌을때 optimal policy 를 계산하는 것은 non-trivial 합니다. 단순한 경우를 제외하고는 일반적으로 강화 학습을 사용하여 policy를 근사합니다 (그림 1 참조). policy는 일반적으로 잡음이있는 것으로 가정합니다 (예 : deterministic 한것 대신 softmax 사용). \n",
    "\n",
    "### motivation\n",
    "\n",
    "- 1) 인간이나 동물의 학습과정을 compuational modeling 하기 위해\n",
    "\n",
    "- 2) intelligent agent 를 만들기 위해\n",
    "\n",
    "    - we propose to recover the expert's reward function and to use this to generate desirable behiavior\n",
    "    - RL 문제에서 reward function에 관심을 두는 것이 policy에 관심을 두는 것보다 succinct, robust, transferable 하다.\n",
    "    \n",
    "    \n",
    "Owain 과 나는 AI가 인간의 가치를 배우는 방법으로 제안되었기 때문에 IRL에 특히 관심이 많습니다. 이는 결국에는 learining을 효율적으로 하고 인간의 value를 구현함으로써 agent를 파워풀 하게 해줄 것이고 더 넓은 scope에서 행동할 수 있게 할 것입니다.우리는 IRL 이 이를 고려하기에 적합하다고 생각하지만 잘못 설계한 모델은 IRL의 성능과 밀접한 관계가 있다고 생각합니다. IRL의 역할은 인간의 value를 추론하는 것이고 이는 reward function ro utility function으로 표현되어 집니다. 하지만 문제는 human value는 직접적으로 관찰되어지지 않습니다.\n",
    "    \n",
    "# 2, Notation and problem Formation\n",
    "\n",
    "역 강화 학습에서는 주어진 환경에서 행동을 취하는 에이전트를 모델링하려고합니다. 그러므로 우리는 <b>state space</b> S(agent와 environment가 가질 수 있는 state의 집합), <b>action space</b> A(agent가 취할 수 있는 action의 집합), 현재 state에서 다음 state로 갈 확률을 제공 하는 <b>transition function</b>  $T (s '\\mid s, a)$ 를 가정합니다. 예를 들어, 자동차를 제어하는 AI 학습의 경우, state space은 자동차의 가능한 위치 및 방향이며, action space은 AI가 자동차에 보낼 수있는 제어 신호 세트 일 것이며, transition function은 자동차의 동력학 모델이되어야합니다. 앞의 세개의 집합인 튜플 (S, A, T) 는 MDP\\R,라고 불리며 이는 보상 기능이없는 마르코프 결정 프로세스입니다. ( MDP\\R 알려진 horizon 또는 discount factor $\\gamma$을 갖지만 단순화를 위해이를 남겨 둘 것입니다.)\n",
    "\n",
    "<img src=\"./image01.png\">\n",
    "\n",
    "그림 1 : IRL과 RL의 관계를 보여주는 다이어그램 (제공 : Pieter Abbeel의 IRL 슬라이드 )\n",
    "\n",
    "IRL의 문제는 optimal policy  $ \\pi^* : S -> A \\ $이 주어졌을때 reward function R 을 추론하는 것입니다.  우리는 $ \\pi ^ *$  을 (s, a) 로 부터 학습하고 $ \\pi ^ * $ 로 부터 대응되는 action을 학습합니다. 일반적으로 이런 (s,a) 는 trajectory(single episode에서 나온 agent의 action, state)부터 뽑아 낼 수 있습니다.\n",
    "\n",
    "$ (s_0, a_0), (s_1, a_1), \\ldots, (s_n, a_n) $ \n",
    "\n",
    "\n",
    "자동차 예에서 이것은 원하는 운전 행동 (운전대가 조향 휠, 브레이크 등에 신호로 기록되는 곳)을 시연하는 노련한 운전자가 취한 행동과 일치합니다.  \n",
    "\n",
    "\n",
    "MDP \\R 과 어떤 trajectory 가 주어졌을때 IRL의 목표는 reward function 을 추론하는 것입니다. 베이지안 프레임 워크에서 어떤 R의 Specify 한 사전분포를 지정하면 밑처럼 나타낼 수 있습니다.\n",
    "\n",
    "\n",
    "$P(R \\mid s_{0:n},a_{0:n}) \\propto P( s_{0:n},a_{0:n} \\mid R) P(R) = P(R) \\cdot \\prod_{i=0}^n P( a_i \\mid s_i, R)$\n",
    "\n",
    "\n",
    "The likelihood $P(a_i \\mid s_i, R)$ is just $\\pi_R(s)[a_i]$, where $\\pi_R$ is the optimal policy under the reward function R.\n",
    "\n",
    "reward가 주어졌을때 optimal policy 를 계산하는 것은 non-trivial 합니다. 단순한 경우를 제외하고는 일반적으로 강화 학습을 사용하여 policy를 근사합니다 (그림 1 참조). policy는 일반적으로 잡음이있는 것으로 가정합니다 (예 : deterministic 한것 대신 softmax 사용). 사전분포를 지정하는 것의 어려움 때문에, optimal polciy, intergrating over reward function을 하는 것에 일반적으로 베이지안 방법을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRL\n",
    "\n",
    "IRL의 문제는 관측된 행동을 설명하는 reward function을 찾는 것이다. optimal or expert policy $\\pi$ 가 주어져 있고 그 policy 의 action을 a1 로 naming 했다.\n",
    "\n",
    "## Thm\n",
    "\n",
    "모든 reward function은 밑의 수식을 만족한다.   \n",
    "이 수식은 optimal policy 로 부터 얻는 value > other policy 로 부터 얻는 value 로 부터 유도되어져 나온다.\n",
    "<img src=\"./IRL_thm.png\">\n",
    "\n",
    "reward function이 만족해야 하는 수식을 찾았는데.. 사실 이를 만족하는 reward function은 무수히 많을 것이다. 이들중에서 어떤 것을 optimal reward function으로 선택해야 할 것인가?\n",
    "\n",
    "하나의 자연스러운 방법으로는 best action과 next-best action 을 최대로 하는 것을 원한다.\n",
    "\n",
    "## $ \\sum_{s}({Q^{\\pi}(s,a1) } - \\max_{a} {Q^{\\pi}(s,a)}) $\n",
    "\n",
    "위의 개념을 thm에 적용해 best action 과 next-best action의 차이를 최대로 하는 reward function을 찾는 것이 우리의 목표이고 이를 수식으로 나타내면 밑과 같다\n",
    "\n",
    "<img src=\"./optimazation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear function approximation in state space\n",
    "\n",
    "we assume that we have the ability to simulate trajecories in the MDP under the optimal policy pi or under any policy of our choice \n",
    "\n",
    "reward function을 approximation 해보자..\n",
    "\n",
    "\n",
    "\n",
    "## $R(s) = a_{1}\\Phi_{1}(s) + a_{2}\\Phi _{2}(s) \\ldots +  a_{d}\\Phi _{d}(s) $\n",
    "\n",
    "Phi 가 linear 한 특성을 가지고 있다.. Vpi 는 단순히 R에다 어떤것을 곱하고 더한 fomular 이고 이는 결국 밑으로 유도된다.. \n",
    "\n",
    "## $V^{\\pi}  = a_{1}V^{\\pi}_1 + a_{2}V^{\\pi}_2 \\ldots +  a_{d}V^{\\pi}_d $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./approximation.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
