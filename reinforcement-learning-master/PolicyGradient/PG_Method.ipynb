{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# PG_Method의 장단점\n",
    "\n",
    "### 장점\n",
    "\n",
    "- #### 1, value approch 는 deterministic policy를 위해 설계되어져 있다. 반면에 확률적인 policy가 유용 할 경우가 있다.\n",
    "    - ex) 가위바위보 게임 같은 경우 stochastic policy 가 바람직해 보인다.\n",
    "\n",
    "- #### 2, value approch 에서는 value의 약간의 변화만으로도 policy에 큰변화가 생길 수 있다. \n",
    "    - ex) value approch 에서 [25, 25, 24, 26 ] -> [25, 25, 27, 26 ] 으로 update 됐다고 하자.. value는 아주 약간 바뀌엇지만 policy 입장에서는 매우 큰 변화이다.이는 즉 stable 한 learning이 힘들다는 것이다. PG_Method 에서는 stable한 학습이 가능하다. \n",
    "    - ps) 강화학습에서는 stable 한 학습이 매우 큰 이슈이다. doble q learing, dueling q learing 등은 모두 좀 더 stable 한 학습을 위한 알고리즘이다.\n",
    "\n",
    "\n",
    "- #### 3, hig-dim or continuos action space 에 효과적이다.\n",
    "    - value based 에서는 max를 계산해야 하는데 만약 action space가 매우 크다면...?? max연산을 하기 위해서는 action space를 모두 계산을 해야 한다.\n",
    "\n",
    "### 단점 \n",
    "\n",
    "- #### 1, .Typically converge to a local rather than global optimum\n",
    "\n",
    "- #### 2, Evaluating a policy is typically inefficient and high variance\n",
    "\n",
    "    - ps) value based 에서 max가 있던 것을 기억하자. max를 취한 다는 것은 내가 지금까지 학습한 것중에는 이것이 베스트야!! 라고 생각하는 것인데 이는 좀더 aggressive한 방법이다. 즉 policy based 는 좀더 stable 하지만 비효율적이거나 느릴 수 있다\n",
    "\n",
    "\n",
    "# 밑은 Actor - Critic 에서의 pg\n",
    "\n",
    "Critic - update w by linear td(0)  \n",
    "actor - update 세타 by policy gradient\n",
    "\n",
    "# loss function for pg\n",
    "\n",
    "<img src='./picture/pg_loss.png' width=600 height=550 />\n",
    "\n",
    "policy function을 updqte 하려면 loss function이 필요합니다. 정의하는 방법에는 3가지가 있는데\n",
    "\n",
    "1, state value - 똑같은 state에서 게임이 시작되는 상황에서는 처음 state의 value function 을 최대로 만드는 것이 policy function의 목적이 됩니다.\n",
    "\n",
    "2, averate value - 모든 state 에서의 value 들의 합을 최대로 하는 것이 loss의 목적이 됩니다. (잘 쓰이지 않는다고 합니다.)\n",
    "\n",
    "3, average reward per time-step - 위에서 V(s) 의 부분을 쪼개서 나타내면 밑의 수식이 됩니다.\n",
    "\n",
    "\n",
    "<img src='./picture/pg_2.png' width=600 height=550 />\n",
    "\n",
    "세타라는 파라미터를 가진 policy function이 미분가능하다고 가정하면 \n",
    "위의 loss 중에서 1,3 번을 세타에 대해 미분하면 위의 수식이 됩니다.\n",
    "\n",
    "이때 위 수식을 잘 살펴보면 우리가 모르는 2가지 것이 있습니다. d(s) 라는 statinally distribtuion 을 모르고 Q(s,a) 라는 것도 모릅니다.\n",
    "\n",
    "이를 해결 하기 위해 두가지 방법이 있습니다.\n",
    "\n",
    "1, 세타에 대한 미분텀이 d(s) 에 붙어있지 않으므로 sampling 을 통해 gradient 텀을 구할 수 있습니다. \n",
    "\n",
    "이제 policy gradient를 보다 실제적인 응용을 위해 더 깊게 생각해 보자. Policy gradient theorem은 gradient ascent algorithm에서 필요한 performance gradient를 정확히 수식으로 표현해주고 있다. 이제 우리가 필요한 것은 sampling을 통해 얻은 기대치가 이 수식과 같거나 또는 근사치를 주는 어떤 방법을 찾아내는 것이다. Policy gradient theorem의 오른편은 policy $\\pi$를 따르면서 얼마나 자주 어떤 state가 나타나는지를 가중치로 두고 있으며, 이것을 다시 몇번의 time step을 거쳐 그 state에 도달했는지를 나타내는 가중치  $\\gamma^t$가 요구된다. $\\pi$ 를 따른다면 우리는 이것과 비례해서 state를 방문하게 될것이고 $\\gamma^t$를 가중치로 사용하는 expected value를 아래와 같이 생각해 볼 수 있다.\n",
    "\n",
    "<img src='./picture/rl-eq13-51.png' width=600 height=550 />\n",
    "\n",
    "\n",
    "https://jay.tech.blog/2017/01/03/policy-gradient-methods-part-1/\n",
    "\n",
    "2, 앞의 텀은 해결 되었지만 여전히 우리는 Q(s,a) 를 알지 못합니다. 우리는 이를 추정하기 위한 아주 좋은 재료를 가지고 있는데 그것은 바로 actural return R 입니다. actual R를 통해 Q(s,a)를 추정할 것입니다.\n",
    "ps) Q(s,a)를 추정하지 않고 그냥 actual R를 사용하는 알고리즘이 REINFORCE 알고리즘 입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Policy Gradient with Approximation\n",
    "\n",
    "<img src='./picture/pg_approximation.png' width=600 height=550 />\n",
    "\n",
    "(p-f)^2d/dw = 2(p-f)df/dw ~ (p-f)df/dw\n",
    "\n",
    "위에서 봤던 (2) 에 approximation 한 f_w 를 넣어보면 (3) 이라는 수식이 나옵니다.\n",
    "\n",
    "f_w 가 linear 한 feature를 가지려면 (4) 라는 것을 만족해야 한다는 것을 Tsitsiklis 가 알아 냈고 (3), (4)를 결합하면  (5) 이라는 수식이 나옵니다. ( 많이들 하는 것처럼 nonlinear한 feature를 가지는 NN 에서는 practical 하게 작동한다.. 고 하지 않을까 싶습니다.)\n",
    "\n",
    "pf)\n",
    "\n",
    "벡터 u 외적 벡터 v = 0 이면 orthognal 인데.. (6) 수식이 잘생각해보면 외적을 나타내나보다..\n",
    "v=projUv+projUㅗv 어떤 벡터 v = u 에정사영벡터 +  u 에랑 정사영벡터랑 직교하는 벡터 로 나타낼 수 있고 \n",
    "\n",
    "흠.. 모르겠다... 누가 힌트좀 주세요 ㅠ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Application to Deriving Algorithms and Advantages\n",
    "\n",
    "근데 만약에 우리가 softmax function 으로 policy를 approximation 한다면 f_w 는 각 state마다 zero mean 이라는 것 을 제외하면 policy와 같은 feature를 가져야 한다.\n",
    "\n",
    "이는 f_w를 approximation 하는데 더 나은 아이디어를 가져다 주고 이를 advanrage function 이라 한다. A(S,A) = Q(S,A) - V(S) 로 하는 A(S,A)를 학습하는 것이다.\n",
    "\n",
    "# 이게 바로 Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# REINFORCE \n",
    "\n",
    "<img src='./picture/reinforce1.png' width=600 height=550 />\n",
    "\n",
    "REINFORCE algorithm이라고 한다. 이 update rule을 살펴보면, 변화량은 return G_t와 vector(action을 선택할 확률의 gradient를 그 확률자체로 나눈 항)와의 곱과 비례한다. 이 vector는  미래에 state S_t에서 다시 action A_t를 선택할 확률을 증가시키도록 weight space의 변화 방향을 결정한다. update는 return과 비례해서 weight를 증가시키는 반면, action probability와는 반비례한다.\n",
    "\n",
    "블로그에서 가져온 설명인데.. 위의 action probability 와는 반비례한다는 설명이 맞는 건가..??action을 선택할 확률의 gradient를 그 확률자체로 나눈 항 이기 때문에 반비례한다고 쓰신거 같은데.. 그냥 log 한 성질이 좋으니까 일케한거아닌가?\n",
    "\n",
    "주목해야 할 점은 REINFORCE algorithm은 episode가 끝나는 최종 return값을 사용한다는 점이다. 이런 의미에서 REINFORCE는 Monte Carlo algorithm이라고 할 수 있다.\n",
    "\n",
    "# REINFORCE With Baseline\n",
    "<img src='./picture/reinforce_baseline.png' width=600 height=550 />\n",
    "\n",
    "baseline은 update의 예상 value를 변화시키지 않지만, variance에는 큰 영향을 미치게 된다. 어떤 states에서 모든 action이 모두 큰 value를 가지게되면 작은 value를 가진 state와 차별화하기 위해 큰 baseline이 필요하다. 모든 action이 작은 값을 가지는 state에서는 작은 baseline이 필요하다. Baseline을 정하는 자연스러운 방법은 state value $\\hat{v}(S_t , \\boldsymbol{w})$를 사용하는 것이며, $\\boldsymbol{w}$는 value function의 weight vector이다. REINFORCE는 policy weights를 학습하는 Monte Carlo method이기 때문에, state-value weight $\\boldsymbol{w}$를 학습하기 위해서도 Monte Carlo method를 사용하는 것이 자연스러울것 같다. baseline을 사용하는 REINFORCE algorithm의 pseudocode는 아래와 같다.\n",
    "\n",
    "# Actor-Critic PG\n",
    "<img src='./picture/actor_critic_value.png' width=600 height=550 />\n",
    "\n",
    "Actor-critic methods는 위 그림과 같이 value function과 policy를 다루는 별도의 구조를 가져야 하므로 value function과 별도로 policy를 위한 메모리 구조를 가지고 있다. Policy structure는 action을 선택하는데 사용되기때문에 actor라고 하고, value function을 구하기 위한 value estimator는 선택된 action을 평가하는 역할을 하므로 critic이라고 한다. Critic은 actor가 따르고 있는 policy에 대해 비평을 해야하므로 학습과정은 항상 on-policy라고 할 수 있다.  \n",
    "\n",
    "위에서 봤던 REINFORCE  알고리즘은 critic 이 아닌데 이는bootstrapping 을 사용하지 않고 state value 를baseline 으로만사용한다. \n",
    "이전에 부트스트랩핑을 적용함으로써td 에서 했던것 처럼bias 를 높이는대신variance 를 줄일 수있다. \n",
    "\n",
    "# Continuous Actor Critic Solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
