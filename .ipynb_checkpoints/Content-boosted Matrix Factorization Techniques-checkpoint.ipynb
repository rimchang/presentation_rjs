{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Matrix factorization: A brief review\n",
    "\n",
    "## 2.1 Notation\n",
    "\n",
    "$U = \\{ u_1,...,u_N \\}$ - set of user  \n",
    "$I = \\{ i_1,...,i_N \\}$ - set of Item  \n",
    "$R = [r_{ui}]_{NxM}$ - rating matrix\n",
    "\n",
    "r_ui 는 결국 u user가 i item 에게 매긴 rating 이다. 이는 원칙적으로는 실수이지만 실무적으로는 dislike, like 같은 binary 한 값이거나 [1,5] 사이의 어떤 range를 가진 수이다\n",
    "\n",
    "rating matrix R은 매우 Sparse한 데이터이기 때문에 cold start 문제가 발생한다\n",
    "\n",
    "$T = \\{(u,i): r_{ui} is known\\}$ - 알려진 r_ui 들을 T라고 나타낸다.recommender system 의 목적은 알려지지 않은 나머지의 r_ui 들을 예측하는 것이다. 그리고 예측한 rating을 <bold>$\\hat{r_{ui}}$</bold> 라고 표시한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Normalization by ANOVA\n",
    "\n",
    "ANOVA - type model은 유용한 정보를 이끌어 낼 수 있다. 가장 간단한 ANOVA - type model 은 r_ui 를 다음과 같다고 생각한다\n",
    "\n",
    "$r_{ui} = \\mu + \\alpha_{u} + \\beta_{i} + \\epsilon_{ui} $\n",
    "\n",
    "$ \\epsilon_{ui} $ 은 노이즈를 나타내며 \\mu 는 총 평균, 알파와 베타는 각각 user effect, item effect를 나타낸다. 이 두가지 주효과는 몇가지를 알게 해주는데 어떤 아이템들은 다른 아이템들보다 일반적으로 더 좋아하고 some users are simply more difficult to please.\n",
    "\n",
    "보통 mf or nn methods 를 사용하기 전에 이런 ANOVA - type model을 사용하여 normalize 하는 것이 일반적이다. 우리도 모든 mf algorithm에 $r_{ui} - \\mu - \\alpha_{u} - \\beta_{i} $ 를 적용했으며 예측된 rating은 실제로 $r_{ui} + \\mu + \\alpha_{u} + \\beta_{i} $ 이다. $\\hat{r_{ui}}$ 은 mf 알고리즘에 의해 예측된 rating 이며 $\\hat{\\mu},\\hat{ \\alpha},\\hat{\\beta} $ 은 $\\mu, \\alpha,\\beta $의 MLEs 추정치 들이다. notation의 혼동을 줄이기 위하여 우리는 normalization step 을 거친 R 이라도 r_ui 로 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Matrix factorization\n",
    "\n",
    "R에서의 Unkonwn rating을 예측하기 위해서 우리는 mf 알고리즘을 적용했다. 이는 matrix R을 2가지 low-rank 의 , latent feature matrices 로 나타내는 것이다. \n",
    "\n",
    "$ R \\approx \\hat{R} = PQ^T = [p^T_1,p^T_2,...,p^T_N]^T[q_1,q_2,...q_M]$\n",
    "\n",
    "latent feature veotor p_u 는 유저를 나타내는 것이고, q_I 은 아이템을 나타내는 것이다. 이는 모두 K차원이며 k는 m,n보다 작다. 이러면 $\\hat{r_{ui}} = p_u^Tqi$ 로 간단히 나타내진다.\n",
    "\n",
    "직관적으로 k차원의 사상을 생각 해 볼수 있다. 여기서 p_u와 q_i 는 (latent) coordinates for user and item 이다. 개별적으로 우리가 추천을 하기 위해 필요한 모든 정보는 이 사상 속에 담겨져 있다. Latent-coordiante model 은 긴 역사를 가지고 있는으며 pca, 요인분석, 다변량분석 등등이다.\n",
    "\n",
    "수학적으로 factorization 은 밑의 최적화 문제를 푸는 것으로 풀리는데\n",
    "\n",
    "$min_{P,Q} ||R-PQ^T||^2 $ 를 푸는 것으로 solve 되며 Frobenius norm 이라면 overfitting 을 피하기 위해 regularization 을 해주며 그렇게 되면 밑의 수식으로 전개된다.\n",
    "\n",
    "$min_{P,Q} ||R-PQ^T||^2 + \\lambda(||P||^2+||Q||^2)$\n",
    "\n",
    "\n",
    "베이지안의 관점에서 살펴보면 위의 수식의 앞부분은 Gaussian likelihood function 에서 나온 거이고 뒤의 식은 spherical(구체의) Gaussian priors on the user and item feature vectors 에서 나온 것이다. 위의 수식은 MAP 를 통해 최적화 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Relative scaling of penalty terms\n",
    "\n",
    "Feuerverger 은 empirical bayes analysis 를 여기에 적용했는데 이는 각각의 user, item 에 패널티텀 $||P||^2,||Q||^2$을 다르게 적용한 것이다. 실무적으로 이런 충고는 언제나 적용된건 아니였는데 이는 계산량의 소모와 성능이 많이 상승되지는 않았기 때문이다.\n",
    "\n",
    "우리의 작업에서는 두번째 패널티 텀$||Q||^2$ - (factor r >0 에 의해 패널티를 받음) 을 쉽게 찾는 방법을 찾아 내었으며 n(N), n(M)에 상관없이 Q의 패널티 텀이 P의 패널티 텀에 order of magnitude (지수만큼??,,모르겟음)만큼 같다는 것을 찾았다.\n",
    "\n",
    "추가적으로 R의 대부분이 unknown 이기 때문에 우리는 object function의 첫번째 텀을 알려진 entrie T를 가지고 평가했다(Base line 사용했다는 뜻인듯) \n",
    "\n",
    "${minimize}\\, L_{BL}(P,Q) = \\sum {(r_{ui} - \\hat r_{ui})^2 + \\lambda (\\sum_u {||p_u||}^2 + \\sum_i {||q_i||}^2)}$\n",
    "\n",
    "BL의 표시는 Baseline 사용했다는 뜻"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Alternating gradient descent\n",
    "\n",
    "P,Q가 unknown 이기 때문에 convex하지가 않다. 이것은 gradent descent의 다른 대체적인 방법을 사용해야 하는 것을 의미하는데 Q를 고정하고 가장 잘 설명하는 P를 찾고, P를 고정하고 가장 잘 설명하는 Q를 찾는 과정을 반복적으로 왔다갔다 하면서 값을 찾는다. \n",
    "\n",
    "<img src='./ficture/gd_forbl.jpg' width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 SVD and other matrix factorization techniques\n",
    "\n",
    "CF 논문들에서 위의 MF 접근 방법은 보통 SVD 로 불려지는데 이는 엄격하게 말하면 잘못된 말이다. SVD는 아마도 MF 테크닉 중에서 가장 많이 쓰이는 것이다. 같은 것은 아니다.\n",
    "\n",
    "\n",
    "<img src='./ficture/SVD_algoritm.jpg' width=600 />\n",
    "\n",
    "\n",
    "D* 는 대각 행렬이고 P,Q는 직교행렬이다. MF 접근 방식은 위로 요약되는데 이것은 P,Q가 직교하는 것을 필요로 하는 것이 아니다. 이 방식을 사용해서 넷플릭스 경연의 우승자에 따르면 직교행렬이여야 한다는 제약조건이 없으면 this would certainly raise identifiability and degeneracy questions for the optimization problem (뭐 이론적으로는 그러면 안된다는 말인듯) 하지만 실무적으로 초기화를 잘하면 피해갈 수 있다고 한다.\n",
    "\n",
    "Lee, seung은 다른 mf 기술을 만드었는데 non-negative matrix factoriztion(NMF) 라고 한다 이는 non-negativity constraints 가 추가가 된 것이고\n",
    "\n",
    "$P_{UK} , Q_{IK} >=0 for all u,i,k$ 라는 조건이 추가가 된 것이다. NMF 는 underlying structure을 밝히기 위해 이미지나 유전학등 에서 사용되던 것이\n",
    "다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Content-boosted matrix factorization\n",
    "\n",
    "이제 가정을 해보는데 각각의 아이템 I 마다 content vector $a_i = [a_{i1},a_{in},...,a_{iD}]$의 D개의 변수가 존재 한다고 생각해보자 이를 쌓아서 A=[a_id] 라는 메트릭스로 만들 수 있다. 여기서는 간단하게 A가 0,1로 binary 하다고 가정했다. \n",
    "\n",
    "# 3.1 Alignment-biased factorization\n",
    "\n",
    "mf 알고리즘에 A를 포함하기 위한 아이디어중 하나는 두아이템 I_1,I_2 가 적어도 C attribute를 공유 할 것이다. (if two items\n",
    "i and i′ share at least c attributes in common) 이것은 \"common attributes\" 조건이라고 한다. 그러면 이것은 직관적으로 이들의 latent feature vector q_i_1, q_i_2 가 가까울 것이다 latent space 안에서.\n",
    "\n",
    "## 3.1.1 Details\n",
    "\n",
    "mf 접근을 위해 위에서 봤던 $ R \\approx \\hat{R} = PQ^T = [p^T_1,p^T_2,...,p^T_N]^T[q_1,q_2,...q_M]$ 의 수식에서 closeness 라는 것은 latent feature space의 내적으로 모델링 되어 있다. 그러므로 q_i_1, q_i_2 의 \"close\"는 이 둘의 내적을 뜻한다. 즉 내적이 커지면 close(거리?)가 가깝다는 뜻이다. 우리는 여기에 패널티를 추가 할 것이다 이를 alignment penalty 라고 한다. 이를 최적화 하기 위해서\n",
    "\n",
    "binary 한 a_id 는에서 \"common attributes\"조건은 쉽게 표현되는데 $a^T_i1a_i2 >= c$ 즉 둘의 내적이 c보다 크다는 것이다 \n",
    "\n",
    "\n",
    "<img src='./ficture/alignment_mf.jpg' width=600 />\n",
    "\n",
    "내적의 의미가 무엇이지..? proj vector의 크기,,, 헷갈린다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3.1.2 Differential shrinkage effects(미분수축현상?\n",
    "\n",
    "alignment penalty 의 효과는 명쾌한데 몇가지의 attributes를 공유하는items의 중심으로 latent factor 가 수축된다. selective shrinkage effect 라는 것이다.\n",
    "\n",
    "다음은 alignment penalty의 generalized,smoothed 된 버젼을 볼텐데. 거의 비슷하지만 수학적 포뮬러가 다르다. 우리는 각각의 알고리즘의 다른점을 살펴 볼 것인데 이들 알고리즘은 qi를 약간 다른 중심점으로 향하게 하는 것일뿐 아이디어느 비슷하다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.2 A smooth generalization\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
